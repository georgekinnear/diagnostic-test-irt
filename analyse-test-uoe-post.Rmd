---
title: "Analysis of test with partial credit (University of Edinburgh)"
author: "George Kinnear"
date: '2022-01-12'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
editor_options:
  chunk_output_type: console
---

```{r load-packages, message=FALSE, include=FALSE}
library(mirt)      # For IRT analysis
library(psych)     # For factor analysis
library(parameters)# For factor analysis
library(tidyverse) # For data wrangling and visualisation
library(reshape)   # For reshaping nested lists
library(vroom)     # For reading in many files at once
library(broom)     # For tidying model output
library(fs)        # For file system operations
library(gt)        # For formatting tables
library(kableExtra)# For formatting tables
library(knitr)     # For markdown tables
library(ggrepel)   # For labelling points without overlap
library(skimr)     # For data frame level summary
library(ggridges)  # For ridge plots
library(plotly)    # For interactive plots

# define a colour palette for the MATH taxonomy
MATH_colours <- c("A" = "#D81C3F", "B" = "#0070C0", "C" = "#00B050")
```

# 1. Data

Information about the test:

```{r load-test-info, echo=FALSE, message=FALSE}
item_info <- read_csv("data-uoe/edinburgh-diagtest-metadata.csv") %>% 
  select(question = post, description, MATH_group) %>% 
  filter(!is.na(question)) %>% 
  arrange(parse_number(question))

item_info %>%
  gt() %>%
  data_color(
    columns = contains("MATH"),
    colors = MATH_colours
  )
```

Load the student scores for the test:

```{r load-all-data, echo=FALSE, message=FALSE}
# Moodle records "-" as the result for questions that were not attempted,
# so we set vroom's na option to interpret this as NA
test_scores <-
  vroom(
    fs::dir_ls(path = "data-uoe", glob = "*diagtestSep*"),
    # Moodle records "-" as the result for questions that were not attempted,
    # so we set vroom's na option to interpret this as NA
    na = c("-", "", "NA"), 
    id = "cohort" 
  ) %>%
  mutate(year = str_remove(cohort, "data-uoe/ANON_") %>% str_remove(., "diagtestSep.csv")) %>% 
  rename(Total = `Grade.100.00`) %>% 
  rename_with(.cols = starts_with("Q"), .fn = ~ str_replace(., "Q", "B") %>% str_remove(., "\\.\\.5\\.00")) %>% 
  filter(!is.na(Total)) %>%
  select(year, AnonID, Total, starts_with("B"))

test_scores_unfiltered <- test_scores
```

<details>
    <summary>Show data summary</summary>
```{r data-peek}
test_scores %>% skim()
```
</details>


## Data cleaning

1. For students who took the test more than once, consider the attempt with the highest scores only and remove the others;

2. Eliminate the students who scored three or more zeros in the 5 easiest questions in the
second-half of the test; and

3. Add the students scoring more than 30 marks in total back to the sample.

```{r}
test_scores <- test_scores_unfiltered %>% 
  group_by(AnonID) %>% 
  slice_max(Total, n = 1) %>% 
  ungroup() %>% 
  rowwise() %>% 
  mutate(invalid_in_easiest_5 = sum(is.na(B11), is.na(B12), is.na(B16), is.na(B17), is.na(B18))) %>% 
  filter(invalid_in_easiest_5 <= 2 | Total >= 30) %>% 
  select(-invalid_in_easiest_5) %>% 
  ungroup()

# test_scores <- test_scores_unfiltered %>%
#   group_by(AnonID) %>%
#   slice_max(Total, n = 1) %>%
#   ungroup() %>%
#   filter(Total > 0)

bind_rows(
  "unfiltered" = test_scores_unfiltered %>% select(Total),
  "filtered" = test_scores %>% select(Total),
  .id = "dataset"
) %>% 
  mutate(dataset = fct_relevel(dataset, "unfiltered", "filtered")) %>% 
  ggplot(aes(x = Total)) +
  geom_histogram(bins = 25) +
  facet_wrap(vars(dataset)) +
  theme_minimal()
```

```{r}
test_scores %>% 
  mutate(num_na = rowSums(is.na(.))) %>% 
  janitor::tabyl(num_na)

test_scores %>% 
  mutate(num_na = rowSums(is.na(.))) %>% 
  summarise(total_na = sum(num_na), na_rate = total_na / (n() * 20))

```

```{r}
test_scores <- test_scores %>% mutate(across(starts_with('B') & where(is.numeric), ~replace_na(.,0)))
```


## Data summary

The number of responses from each class:

```{r skim-classes}
test_scores %>% 
  group_by(year) %>% 
  tally() %>% 
  gt() %>% 
  data_color(
    columns = c("n"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  )
```

Mean and standard deviation for each item:

```{r skim-all-data}
test_scores %>% 
  select(-AnonID, -Total) %>% 
  group_by(year) %>% 
  skim_without_charts() %>% 
  select(-contains("character."), -contains("numeric.p"), -skim_type) %>% 
  rename(complete = complete_rate) %>% 
  # make the table wider, i.e. with separate columns for each year's results, with the year at the start of each column name
  pivot_wider(names_from = year, values_from = -c(skim_variable, year), names_glue = "{year}__{.value}") %>% 
  # put the columns in order by year
  select(sort(names(.))) %>% 
  select(skim_variable, everything()) %>% 
  # use GT to make the table look nice
  gt(rowname_col = "skim_variable") %>% 
  # group the columns from each year
  tab_spanner_delim(delim = "__") %>%
  fmt_number(columns = contains("numeric"), decimals = 2) %>%
  fmt_percent(columns = contains("complete"), decimals = 0) %>% 
  # change all the numeric.mean and numeric.sd column names to Mean and SD
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.mean"), label = "Mean") %>% deframe()
  ) %>% 
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.sd"), label = "SD") %>% deframe()
  ) %>%
  data_color(
    columns = contains("numeric.mean"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  )
```



# 2. Testing assumptions

Before applying IRT, we should check that the data satisfies the assumptions needed by the model. In particular, to use a 1-dimensional IRT model, we should have some evidence of unidimensionality in the test scores.

### Inter-item correlations

If the test is unidimensional then we would expect student scores on pairs of items to be correlated.

This plot shows the correlations between scores on each pair of items:

```{r corr-plot}
item_scores <- test_scores %>% 
  select(starts_with("B"), -AnonID)

cor_ci <- psych::corCi(item_scores, plot = FALSE)

psych::cor.plot.upperLowerCi(cor_ci)
```

Checking for correlations that are not significantly different from 0, there are none:

```{r cor-not-corr}
cor_ci$ci %>% 
  as_tibble(rownames = "corr") %>% 
  filter(p > 0.05) %>% 
  arrange(-p) %>% 
  select(-contains(".e")) %>% 
  gt() %>% 
  fmt_number(columns = 2:4, decimals = 3)
```

The overall picture is that the item scores are well correlated with each other.

### Dimensionality

```{r fa-checks}
structure <- check_factorstructure(item_scores)
n <- n_factors(item_scores)
```

```{r fa-structure-check, echo=FALSE, results="asis"}
# check_factorstructure(item_scores)

# HACK - to make the heading printed by easystats be h4 rather than h1, use ### 
# TODO - perhaps suggest modification to https://github.com/easystats/insight/blob/master/R/print.easystats_check.R and https://github.com/easystats/parameters/blob/cbbe89c469148735110d5c16ef153e72a20bb0a0/R/n_factors.R#L352

res <- capture.output(structure)
cat(paste0("###", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors, echo=FALSE, results = "asis"}
res <- capture.output(n)
cat(paste0("###", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors-details}
plot(n)
summary(n) %>% gt()
#n %>% tibble() %>% gt()
```


```{r fa-scree}
fa.parallel(item_scores, fa = "fa")
```
```{r include=FALSE}
pdf(file = "output/uoe_post_scree.pdf", width = 6, height = 4)
fa.parallel(item_scores, fa = "fa")
dev.off()
```

### 1 Factor

We use the `factanal` function to fit a 1-factor model.

_Note that this function cannot handle missing data, so any `NA` scores must be set to `0` for this analysis._

```{r fa1}
fitfact <- factanal(item_scores,
                    factors = 1,
                    rotation = "varimax")
print(fitfact, digits = 2, cutoff = 0.3, sort = TRUE)

load <- tidy(fitfact)

load %>% 
  select(question = variable, factor_loading = fl1) %>% 
  left_join(item_info, by = "question") %>% 
  ggplot(aes(x = factor_loading, y = 0, colour = MATH_group)) + 
    geom_point() + 
    geom_label_repel(aes(label = question), show.legend = FALSE) +
    scale_colour_manual(values = MATH_colours) +
  scale_y_discrete() +
    labs(x = "Factor 1", y = NULL,
         title = "Standardised Loadings", 
         subtitle = "Based upon correlation matrix") +
    theme_minimal()
```

```{r}
load %>% 
  select(question = variable, factor_loading = fl1) %>% 
  left_join(item_info, by = "question") %>% 
  arrange(-factor_loading) %>% 
  gt() %>%
  data_color(
    columns = contains("factor"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("MATH"),
    colors = MATH_colours
  )
```

It is striking here that the MATH Group B questions are those that load most strongly onto this factor.


### 2 Factor

Here we also investigate the 2-factor solution, to see whether these factors are interpretable.

```{r fa-alt}
fitfact2 <- factanal(item_scores, factors = 2, rotation = "varimax")
print(fitfact2, digits = 2, cutoff = 0.3, sort = TRUE)

load2 <- tidy(fitfact2)

load2_plot <- load2 %>%
  rename(question = variable) %>% 
  left_join(item_info, by = "question") %>%
  ggplot(aes(x = fl1, y = fl2, colour = MATH_group, shape = MATH_group)) +
  geom_point() +
  geom_text_repel(aes(label = question), show.legend = FALSE, alpha = 0.6) +
  labs(
    x = "Factor 1 (of 2)",
    y = "Factor 2 (of 2)"
  ) +
  scale_colour_manual("MATH group", values = MATH_colours) +
  scale_shape_manual(name = "MATH group", values = c(19, 17, 18)) +
  theme_minimal()

load2_plot +
  labs(
    title = "Standardised Loadings",
    subtitle = "Showing the 2-factor model"
  )

ggsave("output/uoe_post_2factor.pdf", units = "cm", width = 12, height = 8, dpi = 300,
       plot = load2_plot)
```

```{r}
main_factors <- load2 %>% 
#  mutate(factorNone = 0.4) %>%  # add this to set the main factor to "None" where all loadings are below 0.4
  pivot_longer(names_to = "factor",
               cols = contains("fl")) %>% 
  mutate(value_abs = abs(value)) %>% 
  group_by(variable) %>% 
  top_n(1, value_abs) %>% 
  ungroup() %>% 
  transmute(main_factor = factor, variable)


load2 %>% 
  select(-uniqueness) %>% 
  # add the info about which is the main factor
  left_join(main_factors, by = "variable") %>%
  left_join(item_info %>% select(variable = question, description, MATH_group), by = "variable") %>% 
  arrange(main_factor) %>% 
  select(main_factor, everything()) %>% 
  # arrange adjectives by descending loading on main factor
  rowwise() %>% 
  mutate(max_loading = max(abs(c_across(starts_with("fl"))))) %>% 
  group_by(main_factor) %>% 
  arrange(-max_loading, .by_group = TRUE) %>% 
  select(-max_loading) %>% 
  # sort out the presentation
  rename("Main Factor" = main_factor,
         "Question" = variable) %>%
  mutate_at(
    vars(starts_with("fl")),
    ~ cell_spec(round(., digits = 3), bold = if_else(abs(.) > 0.4, T, F))
  ) %>% 
  kable(booktabs = T, escape = F, longtable = T) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>%
  kableExtra::kable_styling(latex_options = c("repeat_header"))
```

> **TODO**


# 3. Fitting IRT model

The `mirt` implementation of the graded partial credit model (`gpmc`) requires that the partial marks are consecutive integers. We therefore need to work around this by adjusting our scores into that form (e.g. replacing scores of 0, 2.5, 5 with 1, 2, 3), while keeping track of the true scores attached to each mark level so that we can properly compute expected scores later on.

```{r}
# Determine the mark levels for each item
mark_levels <- item_scores %>% 
  pivot_longer(everything(), names_to = "item", values_to = "score") %>% 
  distinct() %>% 
  arrange(parse_number(item), score) %>% 
  group_by(item) %>%
  mutate(order = row_number()) %>% 
# Note that the convention used by mirt is for items that have only 2 levels (i.e. 0 marks or full marks),
# the columns are P.0 and P.1, while other items are indexed from 1, i.e. P.1, P.2, ...
# https://github.com/philchalmers/mirt/blob/accd2383b9a4d17a4cab269717ce98434900b62c/R/probtrace.R#L57
  mutate(level = case_when(
    max(order) == 2 ~ order - 1,
    TRUE ~ order * 1.0
  )) %>% 
  mutate(levelname = paste0(item, ".P.", level))

# Use the mark_levels table to replace scores with levels
# (first pivot the data to long form, make the replacement, then pivot back to wide again)
item_scores_levelled <- item_scores %>% 
  # temporarily add row identifiers
  mutate(row = row_number()) %>% 
  pivot_longer(cols = -row, names_to = "item", values_to = "score") %>% 
  left_join(mark_levels %>% select(item, score, level), by = c("item", "score")) %>% 
  select(-score) %>% 
  pivot_wider(names_from = "item", values_from = "level") %>% 
  select(-row)
```


<details>
    <summary>Show model fitting output</summary>
```{r fit-mirt, warning=FALSE, message=FALSE}
fit_gpcm <- mirt(
  data = item_scores_levelled, # just the columns with question scores
  model = 1,          # number of factors to extract
  itemtype = "gpcm",  # generalised partial credit model
  SE = TRUE           # estimate standard errors
  )
```
</details>


## Local independence

We compute Yen's $Q_3$ (1984) to check for any dependence between items after controlling for $\theta$. This gives a score for each pair of items, with scores above 0.2 regarded as problematic (see DeMars, p. 48).

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
residuals <- residuals(fit_gpcm, type = "Q3") %>% data.frame
```
```{r}
residuals  %>% as.matrix() %>% 
  corrplot::corrplot(type = "upper")
```

This shows that most item pairs are independent, with only one pair showing cause for concern:

```{r}
residuals %>%
  rownames_to_column(var = "item1") %>%
  as_tibble() %>% 
  pivot_longer(cols = starts_with("B"), names_to = "item2", values_to = "Q3_score") %>% 
  filter(abs(Q3_score) > 0.2) %>% 
  filter(parse_number(item1) < parse_number(item2)) %>%
  gt()
```

Items A16 and A17 are on the chain rule (e.g. differentiating $\cos(4x^2+5)$ and $(3x^2-8)^3$ respectively), so it is perhaps unsurprising that students' performance on these items is not entirely independent.

Given that this violation of the local independence assumption is very mild, we proceed using this model.

## Model parameters

We augment the data with estimated abilities for each student, using mirt's `fscores()` function.

```{r augment-with-f1}
test_scores_with_ability <- test_scores %>%
  mutate(F1 = fscores(fit_gpcm, method = "EAP"))
```

Next, we extract the IRT parameters.

```{r extract-coefs}
coefs_gpcm <- coef(fit_gpcm, IRTpars = TRUE)
```

We use a modified version of the `tidy_mirt_coeffs` function to get all the parameter estimates into a tidy table:

```{r map-tidy-mirt-coefs}
tidy_mirt_coefs <- function(x){
  x %>%
    # melt the list element
    melt() %>%
    # convert to a tibble
    as_tibble() %>%
    # convert factors to characters
    mutate(across(where(is.factor), as.character)) %>%
    # only focus on rows where X2 is a, or starts with b (the parameters in the GPCM)
    filter(X2 == "a" | str_detect(X2, "^b")) %>%
    # in X1, relabel par (parameter) as est (estimate)
    mutate(X1 = if_else(X1 == "par", "est", X1)) %>%
    # turn into a wider data frame
    pivot_wider(names_from = X1, values_from = value) %>% 
    rename(par = X2)
}

# use head(., -1) to remove the last element, `GroupPars`, which does not correspond to a question
tidy_gpcm <- map_dfr(head(coefs_gpcm, -1), tidy_mirt_coefs, .id = "Question")
```

```{r}
tidy_gpcm %>% 
  filter(par == "a") %>% 
  select(-par) %>% 
  rename_with(.fn = ~ paste0("a_", .x), .cols = -Question) %>% 
  left_join(
    tidy_gpcm %>% 
      filter(str_detect(par, "^b")),
    by = "Question"
  ) %>% 
  gt(groupname_col = "Question") %>%
  fmt_number(columns = contains("est|_"), decimals = 3) %>%
  data_color(
    columns = contains("a_"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = c("est", "CI_2.5", "CI_97.5"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  ) %>%
  tab_spanner(label = "Discrimination", columns = contains("a_")) %>%
  tab_spanner(label = "Difficulty", columns = c("par", "est", "CI_2.5", "CI_97.5"))
```


```{r save-results}
tidy_gpcm %>% 
  write_csv("output/uoe_post_gpcm-results.csv")
```


## Information curves

```{r}
theta <- seq(-6, 6, by=0.05)

info_matrix <- testinfo(fit_gpcm, theta, individual = TRUE)
colnames(info_matrix) <- item_info %>% pull(question)
item_info_data <- info_matrix %>% 
  as_tibble() %>% 
  bind_cols(theta = theta) %>% 
  pivot_longer(cols = -theta, names_to = "item", values_to = "info_y") %>% 
  left_join(item_info %>% select(item = question, MATH_group), by = "item") %>% 
  mutate(item = fct_reorder(item, parse_number(item)))
```

### Test information curve

```{r}
item_info_data %>% 
  group_by(theta) %>% 
  summarise(info_y = sum(info_y)) %>% 
  ggplot(aes(x = theta, y = info_y)) +
  geom_line() +
  labs(y = "Information") +
  theme_minimal()

ggsave("output/uoe_post_info.pdf", width = 6, height = 4, units = "in")
```

This shows that the information given by the test is skewed toward the lower end of the ability scale - i.e. it can give more accurate estimates of students' ability where their ability level is slightly below the mean.

### Item information curves

Breaking this down by question helps to highlight those questions that are most/least informative:

```{r}
item_info_data %>% 
  ggplot(aes(x = theta, y = info_y, colour = item)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  facet_wrap(vars(item)) +
  labs(y = "Information") +
  theme_minimal()
```

We can also compute the sums of different subsets of the information curves -- here, we will look at the questions based on their MATH group:

```{r fig.height=3, fig.width=4}
item_info_data %>% 
  group_by(theta) %>% 
  summarise(
    items_all = sum(info_y),
    items_A = sum(ifelse(MATH_group == "A", info_y, 0)),
    items_B = sum(ifelse(MATH_group == "B", info_y, 0)),
    items_C = sum(ifelse(MATH_group == "C", info_y, 0))
  ) %>% 
  pivot_longer(cols = starts_with("items_"), names_to = "items", names_prefix = "items_", values_to = "info_y") %>% 
  mutate(items = fct_relevel(items, "all", "A", "B", "C")) %>% 
  ggplot(aes(x = theta, y = info_y, colour = items)) +
  geom_line() +
  scale_colour_manual(values = c("all" = "#000000", MATH_colours)) +
  labs(x = "Ability", y = "Information") +
  theme_minimal()

ggsave("output/uoe_post_info-curves_A-vs-B.pdf", units = "cm", width = 14, height = 6)
```

This shows that the information in the MATH Group B questions is at a higher point on the ability scale than for the MATH Group A questions.

Since the number of items in each case is different, we consider instead the mean information per item:

```{r fig.height=3, fig.width=4}
item_info_data %>% 
  group_by(theta) %>% 
  summarise(
    items_all = sum(info_y) / n(),
    items_A = sum(ifelse(MATH_group == "A", info_y, 0)) / sum(ifelse(MATH_group == "A", 1, 0)),
    items_B = sum(ifelse(MATH_group == "B", info_y, 0)) / sum(ifelse(MATH_group == "B", 1, 0)),
    items_C = sum(ifelse(MATH_group == "C", info_y, 0)) / sum(ifelse(MATH_group == "C", 1, 0))
  ) %>% 
  pivot_longer(cols = starts_with("items_"), names_to = "items", names_prefix = "items_", values_to = "info_y") %>% 
  mutate(items = fct_relevel(items, "all", "A", "B", "C")) %>% 
  ggplot(aes(x = theta, y = info_y, colour = items)) +
  geom_line() +
  scale_colour_manual(values = c("all" = "#000000", MATH_colours)) +
  labs(x = "Ability", y = "Mean information per item") +
  theme_minimal()

ggsave("output/uoe_post_info-curves_A-vs-B-avg.pdf", units = "cm", width = 10, height = 6)
```

This shows that items of each MATH group are giving broadly similar levels of information on average, but at different points on the ability scale.

## Total information

Using `mirt`'s `areainfo` function, we can find the total area under the information curves.

```{r}
info_gpcm <- areainfo(fit_gpcm, c(-4,4))
info_gpcm %>% gt()
```

This shows that the total information in all items is `r info_gpcm$TotalInfo`.

### Information by item

```{r}
tidy_info <- item_info %>%
  mutate(item_num = row_number()) %>% 
  mutate(TotalInfo = purrr::map_dbl(
    item_num,
    ~ areainfo(fit_gpcm,
               c(-4, 4),
               which.items = .x) %>% pull(TotalInfo)
  ))

tidy_info %>%
  select(-item_num) %>% 
  arrange(-TotalInfo) %>% 
  #group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("info"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("outcome"),
    colors = scales::col_factor(palette = c("viridis"), domain = NULL)
  ) %>%
  cols_label(
    TotalInfo = "Information"
  )
```

Restricting instead to the range $-2\leq\theta\leq2$:

```{r}
tidy_info <- item_info %>%
  mutate(item_num = row_number()) %>% 
  mutate(TotalInfo = purrr::map_dbl(
    item_num,
    ~ areainfo(fit_gpcm,
               c(-2, 2),
               which.items = .x) %>% pull(Info)
  ))

tidy_info %>%
  select(-item_num) %>% 
  arrange(-TotalInfo) %>% 
  #group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("info"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("outcome"),
    colors = scales::col_factor(palette = c("viridis"), domain = NULL)
  ) %>%
  cols_label(
    TotalInfo = "Information"
  )
```
## Response curves

Since the `gpcm` model is more complicated, there is a characteristic curve for each possible score on the question:

```{r}
trace_data <- probtrace(fit_gpcm, theta) %>% 
  as_tibble() %>% 
  bind_cols(theta = theta) %>% 
  pivot_longer(cols = -theta, names_to = "level", values_to = "y") %>% 
  left_join(mark_levels %>% select(item, level = levelname, score), by = "level") %>% 
  mutate(score = as.factor(score))

trace_data %>% 
  mutate(item = fct_reorder(item, parse_number(item))) %>% 
  ggplot(aes(x = theta, y = y, colour = score)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  facet_wrap(vars(item)) +
  labs(y = "Probability of response") +
  theme_minimal()
```

To get a simplified picture for each question, we compute the expected score at each ability level:

```{r}
expected_scores <- trace_data %>% 
  mutate(item = fct_reorder(item, parse_number(item))) %>% 
  group_by(item, theta) %>% 
  summarise(expected_score = sum(as.double(as.character(score)) * y), .groups = "drop")

expected_scores %>% 
  ggplot(aes(x = theta, y = expected_score, colour = item)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  facet_wrap(vars(item)) +
  labs(y = "Expected score") +
  theme_minimal()
```

The resulting curves look quite similar to those from the 2PL, allowing for some similar interpretation. For instance, superimposing all the curves shows that there is a spread of difficulties (i.e. thetas where the expected score is 2.5/5) and that some questions are more discriminating than others (i.e. steeper slopes):

```{r}
plt <- expected_scores %>% 
  ggplot(aes(x = theta, y = expected_score, colour = item, text = item)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  labs(y = "Expected score") +
  theme_minimal()

ggplotly(plt, tooltip = "text")

ggsave(plot = plt, file = "output/uoe_post_iccs-superimposed.pdf", width = 20, height = 14, units = "cm")
```


### Test response curve

```{r}
total_expected_score <- expected_scores %>% 
  group_by(theta) %>% 
  summarise(expected_score = sum(expected_score))

total_expected_score %>% 
  ggplot(aes(x = theta, y = expected_score)) +
  geom_line() +
  # geom_point(data = total_expected_score %>% filter(theta == 0)) +
  # ggrepel::geom_label_repel(data = total_expected_score %>% filter(theta == 0), aes(label = round(expected_score, 1)), box.padding = 0.5) +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  labs(y = "Expected score") +
  theme_minimal()
```


## Packages

In this analysis we used the following packages. You can learn more about each one by clicking on the links below.

- [**mirt**](https://cran.r-project.org/web/packages/mirt/mirt.pdf): For IRT analysis
- [**psych**](https://personality-project.org/r/psych/): For factor analysis
- [**tidyverse**](https://tidyverse.org/): For data wrangling and visualisation
- [**reshape**](http://had.co.nz/reshape/): For reshaping nested lists
- [**vroom**](https://vroom.r-lib.org/): For reading in many files at once
- [**broom**](https://broom.tidymodels.org/): For tidying model output
- [**fs**](https://fs.r-lib.org/): For file system operations
- [**gt**](https://gt.rstudio.com/): For formatting tables
- [**knitr**](https://yihui.org/knitr/): For markdown tables
- [**ggrepel**](https://ggrepel.slowkow.com/): For labelling points without overlap
- [**skimr**](https://docs.ropensci.org/skimr/): For data frame level summary
- [**ggridges**](https://wilkelab.org/ggridges/): For ridge plots


