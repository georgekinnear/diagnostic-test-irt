---
title: "Analysis of multiple-choice test (ETH Zurich)"
author: "Mine Ã‡etinkaya-Rundel, George Kinnear"
date: '2022-06-13'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    number_sections: true
editor_options:
  chunk_output_type: console
---

```{r load-packages, message=FALSE, include=FALSE}
library(mirt)      # For IRT analysis
library(psych)     # For factor analysis
library(parameters)# For factor analysis
library(tidyverse) # For data wrangling and visualisation
library(reshape)   # For reshaping nested lists
library(vroom)     # For reading in many files at once
library(broom)     # For tidying model output
library(fs)        # For file system operations
library(gt)        # For formatting tables
library(knitr)     # For markdown tables
library(kableExtra)# For extra formatting of tables
library(ggrepel)   # For labelling points without overlap
library(skimr)     # For data frame level summary
library(ggridges)  # For ridge plots
library(plotly)    # For interactive plots
library(patchwork) # For combining plots
```

# Data

Information about the test:

```{r load-test-info, echo=FALSE, message=FALSE}
item_info <- read_csv("data-eth/eth-metadata.csv") %>% 
  select(question = pre, description) %>% 
  filter(!is.na(question))

item_info %>%
  gt()
```

Load the student scores for the test - here we load the 2017 and 2018 ETH Zurich test data:

```{r load-all-data, echo=FALSE, message=FALSE}
# produce list of all the relevant file names
# (match only "all.csv" in the "-q36" versions of the test)
files <- dir_ls("data-eth/", recurse = TRUE, regexp = "-q36/s21t-000.*.csv")

# read all files and add a column called file_path to identify them
eth_entry_test <- vroom(files, id = "file_path")

# parse file_path column into year and class
test_scores <- eth_entry_test %>%
  mutate(
    file_path = str_remove(file_path, "data-eth/"),
    file_path = str_remove(file_path, "-q36"),
    file_path = str_remove(file_path, ".csv"),
    ) %>%
  separate(file_path, c("year", "class"), sep = "/")
```

```{r data-peek}
test_scores
```

## Missing data

The data includes scores of `2` for the "I don't know" answer option. We replace these with 0, reflecting a non-correct answer:

```{r}
test_scores <- test_scores %>% 
  mutate(across(starts_with("A"), ~ ifelse(. == 2, 0, .)))
```

## Data summary

The number of responses from each class:

```{r skim-classes}
test_scores <- test_scores %>% 
  rowwise() %>% 
  mutate(Total = rowSums(across(starts_with("A"))))

test_scores_summary <- test_scores %>% 
  group_by(year) %>% 
  summarise(
    n = n(),
    mean = mean(Total),
    sd = sd(Total),
    median = median(Total)
  )

test_scores_summary %>% 
  gt() %>% 
  fmt_number(columns = c("mean", "sd"), decimals = 2) %>%
  data_color(
    columns = c("n"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  )
```

```{r}
p1 <- test_scores %>% 
  ggplot(aes(x = Total)) +
  geom_histogram(binwidth = 2) +
  #scale_x_continuous(limits = c(0,100), breaks = c(0, 50, 100)) +
  facet_grid(cols = vars(year)) +
  theme_minimal() +
  labs(x = "Total score (out of 36)",
       y = "Number of students",
       title = "ETH s21t") +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank())

p2 <- test_scores_summary %>% 
  mutate(
    n = str_glue("{n}"),
    mean = str_glue("{round(mean, digits = 1)}"),
    sd = str_glue("{round(sd, digits = 1)}"),
    median = str_glue("{median}")
  ) %>% 
  pivot_longer(c(n, mean, sd, median), names_to = "layer", values_to = "label") %>% 
  mutate(layer = fct_relevel(layer, c("n", "sd", "mean", "median")) %>% fct_rev()) %>% 
  ggplot(aes(x = 80, y = layer, label = label)) +
  geom_text(size = 10 * 5/14, hjust = 1) +
  scale_x_continuous(limits = c(0,100)) +
  facet_grid(cols = vars(year)) +
  labs(y = "", x = NULL) +
  scale_y_discrete(labels = c("n" = "N", "mean" = "Mean", "median" = "Median")) +
  theme_minimal() +
  theme(axis.line = element_blank(), axis.ticks = element_blank(), axis.text.x = element_blank(),
        panel.grid = element_blank(), strip.text = element_blank())

p1 / p2 +  plot_layout(heights = c(5, 2.5))

ggsave("output/eth_pre_data-summary.pdf", units = "cm", width = 8, height = 8)
```

Mean and standard deviation for each item:

```{r skim-all-data}
test_scores %>% 
  select(-class, -Total) %>% 
  group_by(year) %>% 
  skim_without_charts() %>% 
  select(-contains("character."), -contains("numeric.p"), -skim_type) %>% 
  rename(complete = complete_rate) %>% 
  # make the table wider, i.e. with separate columns for each year's results, with the year at the start of each column name
  pivot_wider(names_from = year, values_from = -c(skim_variable, year), names_glue = "{year}__{.value}") %>% 
  # put the columns in order by year
  select(sort(names(.))) %>% 
  select(skim_variable, everything()) %>% 
  # use GT to make the table look nice
  gt(rowname_col = "skim_variable") %>% 
  # group the columns from each year
  tab_spanner_delim(delim = "__") %>%
  fmt_number(columns = contains("numeric"), decimals = 2) %>%
  fmt_percent(columns = contains("complete"), decimals = 0) %>% 
  # change all the numeric.mean and numeric.sd column names to Mean and SD
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.mean"), label = "Mean") %>% deframe()
  ) %>% 
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.sd"), label = "SD") %>% deframe()
  ) %>%
  data_color(
    columns = contains("numeric.mean"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  )
```

# Testing assumptions

Before applying IRT, we should check that the data satisfies the assumptions needed by the model. In particular, to use a 1-dimensional IRT model, we should have some evidence of *unidimensionality* in the test scores.

## Inter-item correlations

If the test is unidimensional then we would expect student scores on pairs of items to be correlated.

This plot shows the correlations between scores on each pair of items:

```{r corr-plot}
item_scores <- test_scores %>% 
  select(-class, -year, -Total)

cor_ci <- psych::corCi(item_scores, plot = FALSE)

psych::cor.plot.upperLowerCi(cor_ci)
```

Almost all correlations are significantly different from 0 - the only one that is not is:

```{r cor-not-corr}
cor_ci$ci %>% 
  as_tibble(rownames = "corr") %>% 
  filter(p > 0.05) %>% 
  arrange(-p) %>% 
  select(-contains(".e")) %>% 
  gt() %>% 
  fmt_number(columns = 2:4, decimals = 3)
```

Thus, the overall picture is that the item scores are well correlated with each other.

## Dimensionality

We use *factor analysis* as another way to assess unidimensionality. We are looking to see whether a 1-factor solution is feasible.

```{r fa-checks}
structure <- check_factorstructure(item_scores)
n <- n_factors(item_scores)
```

```{r fa-structure-check, echo=FALSE, results="asis"}
# check_factorstructure(item_scores)

# HACK - to make the heading printed by easystats be h4 rather than h1, use ### 
# TODO - perhaps suggest modification to https://github.com/easystats/insight/blob/master/R/print.easystats_check.R and https://github.com/easystats/parameters/blob/cbbe89c469148735110d5c16ef153e72a20bb0a0/R/n_factors.R#L352

res <- capture.output(structure)
cat(paste0("##", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors, echo=FALSE, results = "asis"}
res <- capture.output(n)
cat(paste0("###", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors-details, warning=FALSE}
plot(n)
summary(n) %>% gt()
#n %>% tibble() %>% gt()
```


```{r fa-scree}
fa.parallel(item_scores, fa = "fa")

pdf(file = "output/eth_pre_scree.pdf", width = 6, height = 4)
fa.parallel(item_scores, fa = "fa")
dev.off()
```

This shows that both the 6-factor and 1-factor solutions are worth exploring.

### 1 Factor

We use the `factanal` function to fit a 1-factor model.

_Note that this function cannot handle missing data, so any `NA` scores must be set to `0` for this analysis._

<details>
    <summary>Show factanal output</summary>
```{r fa1, cache=TRUE}
fitfact <- factanal(item_scores,
                    factors = 1,
                    rotation = "varimax")
print(fitfact, digits = 2, cutoff = 0.3, sort = TRUE)
```
</details>

```{r}
load <- tidy(fitfact)

ggplot(load, aes(x = fl1, y = 0)) + 
  geom_point() + 
  geom_label_repel(aes(label = paste0("A", rownames(load))), show.legend = FALSE) +
  labs(x = "Factor 1", y = NULL,
       title = "Standardised Loadings", 
       subtitle = "Based upon correlation matrix") +
  theme_minimal()
```

```{r}
load %>% 
  select(question = variable, factor_loading = fl1) %>% 
  left_join(item_info, by = "question") %>% 
  arrange(-factor_loading) %>% 
  gt() %>%
  data_color(
    columns = contains("factor"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  )
```

This factor appears to be dominated by "procedural calculus skills": standard calculation of derivatives, integral and some trigonometry.


### 6 Factor

Here we also investigate the 6-factor solution, to see whether these factors are interpretable.

<details>
    <summary>Show factanal output</summary>
```{r fa4, cache=TRUE}
fitfact6 <- factanal(item_scores, factors = 6, rotation = "varimax")
print(fitfact6, digits = 2, cutoff = 0.3, sort = TRUE)
```
</details>

```{r}
load6 <- tidy(fitfact6)

ggplot(load6, aes(x = fl1, y = fl2)) + 
  geom_point() + 
  geom_label_repel(aes(label = paste0("A", rownames(load))), show.legend = FALSE) +
  labs(x = "Factor 1", y = "Factor 2",
       title = "Standardised Loadings", 
       subtitle = "Based upon correlation matrix") +
  theme_minimal()
```

```{r}
main_factors <- load6 %>% 
#  mutate(factorNone = 0.4) %>%  # add this to set the main factor to "None" where all loadings are below 0.4
  pivot_longer(names_to = "factor",
               cols = contains("fl")) %>% 
  mutate(value_abs = abs(value)) %>% 
  group_by(variable) %>% 
  top_n(1, value_abs) %>% 
  ungroup() %>% 
  transmute(main_factor = factor, variable)

load6 %>% 
  select(-uniqueness) %>% 
  # add the info about which is the main factor
  left_join(main_factors, by = "variable") %>%
  left_join(item_info %>% select(variable = question, description), by = "variable") %>% 
  arrange(main_factor) %>% 
  select(main_factor, everything()) %>% 
  # arrange adjectives by descending loading on main factor
  rowwise() %>% 
  mutate(max_loading = max(abs(c_across(starts_with("fl"))))) %>% 
  group_by(main_factor) %>% 
  arrange(-max_loading, .by_group = TRUE) %>% 
  select(-max_loading) %>% 
  # sort out the presentation
  rename("Main Factor" = main_factor, # the _ throws a latex error
         "Question" = variable) %>%
  mutate_at(
    vars(starts_with("fl")),
    ~ cell_spec(round(., digits = 3), bold = if_else(abs(.) > 0.4, T, F))
  ) %>% 
  kable(booktabs = T, escape = F, longtable = T) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>%
  kableExtra::kable_styling(latex_options = c("repeat_header"))
```

Here, the first factor seems better described by "abstract understanding of calculus". The second is based on "graphical understanding of calculus".

The third and fourth factors are "procedural calculus skills", with the product and quotient rules distinguished in the fourth factor.

The fifth and sixth factors have a geometrical flavour, with the fifth factor in particular being about "vector geometry".

### Conclusion

The 6-factor solution does seem to be interpretable, however the 1-factor solution is still reasonable. In terms of explaining the variance in the data, the 1-factor solution does a good job at capturing a large chunk:

```{r}
# Compute the proportion of variance explained; see https://stackoverflow.com/a/58159992/17459126
x1 <- loadings(fitfact)
propvar1 <- colSums(x1^2)/nrow(x1)

rbind(`Proportion Var` = propvar1, `Cumulative Var` = cumsum(propvar1))

x6 <- loadings(fitfact6)
propvar6 <- colSums(x6^2)/nrow(x6)

rbind(`Proportion Var` = propvar6, `Cumulative Var` = cumsum(propvar6))
```

Therefore, we are justified in proceeding with a unidimensional IRT model.

# Fitting 2 parameter logistic MIRT model

We can fit a Multidimensional Item Response Theory (mirt) model. From the function definition:

```
mirt fits a maximum likelihood (or maximum a posteriori) factor analysis model to any mixture of dichotomous and polytomous data under the item response theory paradigm using either Cai's (2010) Metropolis-Hastings Robbins-Monro (MHRM) algorithm.
```

The process is to first fit the model, and save the result as a model object that we can then parse to get tabular or visual displays of the model that we might want. When fitting the model, we have the option to specify a few arguments, which then get interpreted as parameters to be passed to the model.

<details>
    <summary>Show model fitting output</summary>
```{r fit-mirt, warning=FALSE, message=FALSE, cache=TRUE}
fit_2pl <- mirt(
  data = item_scores, # just the columns with question scores
  model = 1,          # number of factors to extract
  itemtype = "2PL",   # 2 parameter logistic model
  SE = TRUE           # estimate standard errors
  )
```
</details>

## Local independence

We compute Yen's $Q_3$ (1984) to check for any dependence between items after controlling for $\theta$. This gives a score for each pair of items, with scores above 0.2 regarded as problematic (see DeMars, p. 48).

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
residuals_2pl <- residuals(fit_2pl, type = "Q3") %>% data.frame
```
```{r}
residuals_2pl  %>% as.matrix() %>% 
  corrplot::corrplot(type = "upper")
```

This shows that most item pairs are independent, with only one pair showing cause for concern:

```{r}
residuals_2pl %>%
  rownames_to_column(var = "q1") %>%
  as_tibble() %>% 
  pivot_longer(cols = starts_with("A"), names_to = "q2", values_to = "Q3_score") %>% 
  filter(abs(Q3_score) > 0.2) %>% 
  filter(parse_number(q1) < parse_number(q2)) %>%
  gt()
```

Items A18 and A19 are on the product and quotient rules.

Given that this violation of the local independence assumption is very mild, we proceed using this model.

## Model parameters

We then compute factor score estimates and augment the existing data frame with these estimates, to keep everything in one place. To do the estimation, we use the `fscores()` function from the mirt package which takes in a computed model object and computes factor score estimates according to the method specified. We will use the EAP method for factor score estimation, which is the "expected a-posteriori" method, the default. We specify it explicitly below, but the results would have been the same if we omitted specifying the method argument since it's the default method the function uses.

```{r augment-with-f1}
test_scores_with_2pl_ability <- test_scores %>%
  bind_cols(fscores(fit_2pl, method = "EAP"))
```

We can also calculate the model coefficient estimates using a generic function `coef()` which is used to extract model coefficients from objects returned by modeling functions. We will set the `IRTpars` argument to `TRUE`, which means slope intercept parameters will be converted into traditional IRT parameters.

```{r extract-coefs}
coefs_2pl <- coef(fit_2pl, IRTpars = TRUE)
```

The resulting object `coefs` is a list, with one element for each question, and an additional `GroupPars` element that we won't be using. The output is a bit long, so we're only showing a few of the elements here:

```{r peek-coefs}
coefs_2pl[1:3]
# coefs_2pl[35:37]
```

Let's take a closer look at the first element:

```{r coef-1}
coefs_2pl[1]
```

In this output:

* `a` is discrimination
* `b` is difficulty
* endpoints of the 95% confidence intervals are also shown

To make this output a little more user friendly, we should tidy it such that we have a row per question. We'll do this in two steps. First, write a function that tidies the output for one question, i.e. one list element. Then, map this function over the list of all questions, resulting in a data frame.

```{r f-tidy-mirt-coefs}
tidy_mirt_coefs <- function(x){
  x %>%
    # melt the list element
    melt() %>%
    # convert to a tibble
    as_tibble() %>%
    # convert factors to characters
    mutate(across(where(is.factor), as.character)) %>%
    # only focus on rows where X2 is a or b (discrimination or difficulty), or g (guessing parameter in 3PL)
    filter(X2 %in% c("a", "b", "g")) %>%
    # in X1, relabel par (parameter) as est (estimate)
    mutate(X1 = if_else(X1 == "par", "est", X1)) %>%
    # unite columns X2 and X1 into a new column called var separated by _
    unite(X2, X1, col = "var", sep = "_") %>%
    # turn into a wider data frame
    pivot_wider(names_from = var, values_from = value)
}
```

Let's see what this does to a single element in `coefs`:

```{r apply-once-tidy-mirt-coefs}
tidy_mirt_coefs(coefs_2pl[1])
```

And now let's map it over all 32 elements of coefs:

```{r map-tidy-mirt-coefs}
# use head(., -1) to remove the last element, `GroupPars`, which does not correspond to a question
tidy_2pl <- map_dfr(head(coefs_2pl, -1), tidy_mirt_coefs, .id = "Question")
```

A quick peek at the result:

```{r peek-tidy-output}
tidy_2pl
```

And a nicely formatted table of the result:

```{r tabulate-tidy-output}
tidy_2pl %>% 
  # remove the redundant g_ variables which only make sense for the 3PL model
  select(-starts_with("g_")) %>% 
  gt() %>% 
  fmt_number(columns = contains("_"), decimals = 3) %>%
  data_color(
    columns = contains("a_"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("b_"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  ) %>%
  tab_spanner(label = "Discrimination", columns = contains("a_")) %>%
  tab_spanner(label = "Difficulty", columns = contains("b_")) %>%
  cols_label(
    a_est = "Est.",
    b_est = "Est.",
    a_CI_2.5 = "2.5%",
    b_CI_2.5 = "2.5%",
    a_CI_97.5 = "97.5%",
    b_CI_97.5 = "97.5%"
  )
```

```{r}
wright_plot <- tidy_2pl %>% 
  mutate(qnum = parse_number(Question)) %>% 
  ggplot(aes(
    x = a_est,
    y = b_est
  )) +
  geom_errorbar(aes(ymin = b_CI_2.5, ymax = b_CI_97.5), width = 0, alpha = 0.5) +
  geom_errorbar(aes(xmin = a_CI_2.5, xmax = a_CI_97.5), width = 0, alpha = 0.5) +
  geom_text_repel(aes(label = Question), alpha = 0.5) +
  geom_point() +
  theme_minimal() +
  labs(x = "Discrimination",
       y = "Difficulty")

wright_plot
```

```{r save-2pl-results}
tidy_2pl %>% 
  write_csv("output/eth_pre_2pl-results.csv")
```

## Comparing years and classes

Do students from different programmes of study have different distributions of ability?

### Differences between years

Compare the distribution of abilities in the year groups (though in this case there is only one).

```{r viz-years}
ggplot(test_scores_with_2pl_ability, aes(F1, fill = as.factor(year), colour = as.factor(year))) +
  geom_density(alpha=0.5) + 
  scale_x_continuous(limits = c(-3.5,3.5)) +
  labs(title = "Density plot", 
       subtitle = "Ability grouped by year of taking the test", 
       x = "Ability", y = "Density",
       fill = "Year", colour = "Year") +
  theme_minimal()
```

### Differences between classes

Compare the distribution of abilities in the various classes.

```{r viz-classes}
ggplot(test_scores_with_2pl_ability, aes(x = F1, y = class, colour = class, fill = class)) +
  geom_density_ridges(alpha = 0.5) + 
  scale_x_continuous(limits = c(-3.5,3.5)) +
  guides(fill = FALSE, colour = FALSE) +
  labs(title = "Density plot", 
       subtitle = "Ability grouped by class of taking the test", 
       x = "Ability", y = "Class") +
  theme_minimal()
```

## Information curves


```{r}
theta <- seq(-6, 6, by=0.05)

info_matrix <- testinfo(fit_2pl, theta, individual = TRUE)
colnames(info_matrix) <- item_info %>% pull(question)
item_info_data <- info_matrix %>% 
  as_tibble() %>% 
  bind_cols(theta = theta) %>% 
  pivot_longer(cols = -theta, names_to = "item", values_to = "info_y") %>% 
  left_join(item_info %>% select(item = question), by = "item") %>% 
  mutate(item = fct_reorder(item, parse_number(item)))
```


### Test information curve

```{r}
item_info_data %>% 
  group_by(theta) %>% 
  summarise(info_y = sum(info_y)) %>% 
  ggplot(aes(x = theta, y = info_y)) +
  geom_line() +
  labs(x = "Ability", y = "Information", title = "ETH s21t") +
  theme_minimal()

ggsave("output/eth_pre_info.pdf", width = 10, height = 6, units = "cm")
```

This shows that the information given by the test is focused reasonably centrally around the mean ability level.


### Item information curves

Breaking this down by question helps to highlight those questions that are most/least informative:

```{r}
item_info_data %>% 
  ggplot(aes(x = theta, y = info_y, colour = item)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  facet_wrap(vars(item)) +
  labs(y = "Information") +
  theme_minimal()
```

## Total information

Using `mirt`'s `areainfo` function, we can find the total area under the information curves.

```{r}
irt_info <- areainfo(fit_2pl, c(-4,4))
irt_info %>% gt()
```

This shows that the total information in all items is `r irt_info$TotalInfo`.

### Information by item

```{r}
tidy_info <- item_info %>%
  mutate(item_num = row_number()) %>% 
  mutate(TotalInfo = purrr::map_dbl(
    item_num,
    ~ areainfo(fit_2pl,
               c(-4, 4),
               which.items = .x) %>% pull(TotalInfo)
  ))

tidy_info %>%
  select(-item_num) %>% 
  arrange(-TotalInfo) %>% 
  #group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("info"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("outcome"),
    colors = scales::col_factor(palette = c("viridis"), domain = NULL)
  ) %>%
  cols_label(
    TotalInfo = "Information"
  )
```

Restricting instead to the range $-2\leq\theta\leq2$:

```{r}
tidy_info <- item_info %>%
  mutate(item_num = row_number()) %>% 
  mutate(TotalInfo = purrr::map_dbl(
    item_num,
    ~ areainfo(fit_2pl,
               c(-2, 2),
               which.items = .x) %>% pull(Info)
  ))

tidy_info %>%
  select(-item_num) %>% 
  arrange(-TotalInfo) %>% 
  #group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("info"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("outcome"),
    colors = scales::col_factor(palette = c("viridis"), domain = NULL)
  ) %>%
  cols_label(
    TotalInfo = "Information"
  )
```

## Response curves

These show the probability of a correct response at each ability level:

```{r}
trace_data <- probtrace(fit_2pl, theta) %>% 
  as_tibble() %>% 
  bind_cols(theta = theta) %>% 
  pivot_longer(cols = -theta, names_to = "level", values_to = "y") %>% 
  separate(level, into = c("item", NA, "score"), sep = "\\.") %>% 
  mutate(item = fct_reorder(item, parse_number(item))) %>% 
  filter(score == 1)

trace_data %>% 
  ggplot(aes(x = theta, y = y, colour = score)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1, guide = "none") +
  facet_wrap(vars(item)) +
  labs(y = "Probability of correct response") +
  theme_minimal()
```

These can all be overlaid on a single plot:

```{r}
plt <- trace_data %>% 
  ggplot(aes(x = theta, y = y, colour = item, text = item)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  labs(y = "Probability of correct response") +
  theme_minimal()

ggplotly(plt, tooltip = "text")

ggsave(plot = plt, file = "output/eth_pre_iccs-superimposed.pdf", width = 20, height = 14, units = "cm")
```

Here we highlight some items to illustrate the role of the difficulty and discrimination parameters

```{r}
highlight_items <- c("A1", "A2", "A26", "A27")
item_colours <- c(viridis::viridis_pal(option = "plasma", end = 0.8, direction = -1)(length(highlight_items)), "black")

wright_plot_shaded <- tidy_2pl %>%
  mutate(item = Question) %>%
  mutate(
    item_colour = case_when(item %in% highlight_items ~ item,
                            TRUE ~ "ignore"),
    item_size = if_else(item %in% highlight_items, 4, 3)
  ) %>% 
  ggplot(aes(
    x = a_est,
    y = b_est,
    colour = item_colour
  )) +
  geom_errorbar(aes(ymin = b_CI_2.5, ymax = b_CI_97.5), width = 0, alpha = 0.5) +
  geom_errorbar(aes(xmin = a_CI_2.5, xmax = a_CI_97.5), width = 0, alpha = 0.5) +
  geom_text_repel(aes(label = Question, size = item_size), alpha = 0.5) +
  geom_point() +
  scale_colour_manual("item", values = item_colours, limits = highlight_items, guide = "none") +
  scale_size(guide = "none", range = c(2.5, 4.5)) +
  theme_minimal() +
  coord_flip() +
  labs(x = "Discrimination",
       y = "Difficulty")

trace_examples <- trace_data %>% 
  filter(item %in% highlight_items) %>% 
  ggplot(aes(x = theta, y = y, colour = item)) +
  geom_line() +
  # ggrepel::geom_label_repel(
  #   data = trace_data %>% filter(item %in% highlight_items, y > 0.5) %>% group_by(item) %>% slice_min(y, n = 1),
  #   aes(label = item),
  #   box.padding = 0.5,
  #   min.segment.length = Inf,
  #   show.legend = FALSE
  # ) +
  geom_label(
    data = trace_data %>% filter(item %in% c("A1", "A27"), y > 0.45) %>% group_by(item) %>% slice_min(y, n = 1),
    aes(label = item),
    show.legend = FALSE
  ) +
  geom_label(
    data = trace_data %>% filter(item %in% c("A2", "A26"), y > 0.55) %>% group_by(item) %>% slice_min(y, n = 1),
    aes(label = item),
    show.legend = FALSE
  ) +
  scale_colour_viridis_d("item", option = "plasma", end = 0.8, direction = -1, guide = "none") +
  labs(x = "Ability", y = "Probability of correct response") +
  theme_minimal()

combined_plot <- patchwork::wrap_plots(wright_plot_shaded, trace_examples)
ggsave("output/eth_pre_wrightmap_highlight.pdf", plot = combined_plot, units = "cm", width = 18, height = 9)
combined_plot
```


```{r message=FALSE, warning=FALSE}
removed_items <- read_csv("data-eth/eth-metadata.csv") %>% filter(is.na(post)) %>% select(pre) %>% deframe()
highlight_removed_items <- trace_data %>% 
  mutate(highlight_item = item %in% removed_items) %>% 
  mutate(line_width = ifelse(highlight_item, 1, 0.5))

highlight_removed_items %>% 
  ggplot(aes(x = theta, y = y, colour = item, text = item, alpha = highlight_item)) +
  geom_line(aes(size = highlight_item)) +
  #geom_point(data = highlight_removed_items %>% filter(highlight_item == TRUE, theta == 0)) +
  ggrepel::geom_label_repel(
    #data = highlight_removed_items %>% filter(highlight_item == TRUE) %>% group_by(item) %>% mutate(item_num = cur_group_id()) %>% ungroup() %>% mutate(threshold = seq(5, -5, len=max(item_num))[item_num]) %>% filter(theta >= threshold) %>% group_by(item_num) %>% slice(n=1),
    #data = highlight_removed_items %>% filter(highlight_item == TRUE, y >= 0.5) %>% group_by(item) %>% slice_min(y, n = 1),
    data = highlight_removed_items %>% filter(highlight_item == TRUE, theta == 0),
    aes(label = item),
    box.padding = 0.5,
    max.overlaps = Inf,
    min.segment.length = 0,
    show.legend = FALSE
  ) +
  scale_colour_viridis_d("Question", option = "plasma", end = 0.8, direction = -1) +
  scale_size_manual(values = c("FALSE" = 0.6, "TRUE" = 0.9), guide = "none") +
  scale_alpha_discrete(guide = "none", range = c(0.2, 1)) +
  labs(x = "Ability", y = "Expected score") +
  theme_minimal() +
  theme(legend.position="bottom",#legend.title=element_blank(),
      legend.margin = margin(0, 0, 0, 0),
      legend.spacing.x = unit(1, "mm"),
      legend.spacing.y = unit(0, "mm")) +
  guides(colour = guide_legend(nrow = 4))
ggsave(file = "output/eth_pre_iccs-highlight.pdf", width = 16, height = 12, units = "cm")
```


# Checking 3PL

The 3 parameter IRT model adds a third "guessing" parameter for each item, which gives the lower asymptote of the response curve.

Given that the test offered an "I don't know" option (which we have mapped to a score of 0), we would expect that students are less likely to be guessing when answering these multiple-choice questions. So, we would expect to see the lower aymptotes being closer to 0 than would ordinarily be expected for a 4- or 5-option MCQ.


```{r fit-mirt-3pl, warning=FALSE, message=FALSE, results='hide'}
fit_3pl <- mirt(
  data = item_scores, # just the columns with question scores
  model = 1,          # number of factors to extract
  itemtype = "3PL",   # 3 parameter logistic model
  SE = TRUE           # estimate standard errors
  )

residuals_3pl <- residuals(fit_3pl, type = "Q3") %>% data.frame

coefs_3pl <- coef(fit_3pl, IRTpars = TRUE)

test_scores_with_3pl_ability <- test_scores %>%
  bind_cols(fscores(fit_3pl, method = "EAP"))
```

## Local independence

We compute Yen's $Q_3$ (1984) to check for any dependence between items after controlling for $\theta$. This gives a score for each pair of items, with scores above 0.2 regarded as problematic (see DeMars, p. 48).

```{r}
residuals_3pl  %>% as.matrix() %>% 
  corrplot::corrplot(type = "upper")
```

This shows that most item pairs are independent, with only a couple of pairs showing cause for concern:

```{r}
residuals_3pl %>%
  rownames_to_column(var = "q1") %>%
  as_tibble() %>% 
  pivot_longer(cols = starts_with("A"), names_to = "q2", values_to = "Q3_score") %>% 
  filter(abs(Q3_score) > 0.2) %>% 
  filter(parse_number(q1) < parse_number(q2)) %>%
  gt()
```

* Items A18 and A19 are on the product and quotient rules, with an identical structure in each case (giving all the values that need to be used in the relevant formula).

* Items A34 and A35 are are both about planes in vector geometry.

Given that this violation of the local independence assumption is very mild, we proceed using this model.

## Model parameters

```{r map-tidy-mirt-coefs-3pl}
tidy_3pl <- map_dfr(head(coefs_3pl, -1), tidy_mirt_coefs, .id = "Question")
```

A quick peek at the result:

```{r peek-tidy-output-3pl}
tidy_3pl
```

And a nicely formatted table of the result:

```{r tabulate-tidy-output-3pl}
gt(tidy_3pl) %>%
  fmt_number(columns = contains("_"), decimals = 3) %>%
  data_color(
    columns = contains("a_"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("b_"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("g_"),
    colors = scales::col_numeric(palette = c("Reds"), domain = NULL)
  ) %>%
  tab_spanner(label = "Discrimination", columns = contains("a_")) %>%
  tab_spanner(label = "Difficulty", columns = contains("b_")) %>%
  tab_spanner(label = "Guessing", columns = contains("g_")) %>%
  cols_label(
    a_est = "Est.",
    b_est = "Est.",
    g_est = "Est.",
    a_CI_2.5 = "2.5%",
    b_CI_2.5 = "2.5%",
    g_CI_2.5 = "2.5%",
    a_CI_97.5 = "97.5%",
    b_CI_97.5 = "97.5%",
    g_CI_97.5 = "97.5%"
  )
```

```{r plot-tidy-3pl}
tidy_3pl %>% 
  mutate(qnum = parse_number(Question)) %>% 
  ggplot(aes(
    x = qnum,
    y = g_est
  )) +
  geom_errorbar(aes(ymin = g_CI_2.5, ymax = g_CI_97.5), width = 0, alpha = 0.5) +
  geom_text_repel(aes(label = Question), alpha = 0.5) +
  geom_point() +
  theme_minimal() +
  labs(x = "Question",
       y = "Guessing parameter")
```

```{r save-3pl-results}
tidy_3pl %>% 
  write_csv("output/eth_pre_3pl-results.csv")
```

This shows that overall guessing appears to be rare; certainly below the level that might be expected if students of very low ability were simply choosing an answer at random.


# About this report {.unnumbered}

This report supports the analysis in the following paper:

> [citation needed]

## Packages {.unnumbered}

In this analysis we used the following packages. You can learn more about each one by clicking on the links below.

- [**mirt**](https://cran.r-project.org/web/packages/mirt/mirt.pdf): For IRT analysis
- [**psych**](https://personality-project.org/r/psych/): For factor analysis
- [**tidyverse**](https://tidyverse.org/): For data wrangling and visualisation
- [**reshape**](http://had.co.nz/reshape/): For reshaping nested lists
- [**vroom**](https://vroom.r-lib.org/): For reading in many files at once
- [**broom**](https://broom.tidymodels.org/): For tidying model output
- [**fs**](https://fs.r-lib.org/): For file system operations
- [**gt**](https://gt.rstudio.com/): For formatting tables
- [**knitr**](https://yihui.org/knitr/): For markdown tables
- [**ggrepel**](https://ggrepel.slowkow.com/): For labelling points without overlap
- [**skimr**](https://docs.ropensci.org/skimr/): For data frame level summary
- [**ggridges**](https://wilkelab.org/ggridges/): For ridge plots


