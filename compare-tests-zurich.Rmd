---
title: "Compare two versions of a multiple-choice test (ETH Zurich)"
author: "George Kinnear"
date: '2022-01-11'
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r load-packages, message=FALSE, include=FALSE}
library(mirt)      # For IRT analysis
library(psych)     # For factor analysis
library(parameters)# For factor analysis
library(tidyverse) # For data wrangling and visualisation
library(reshape)   # For reshaping nested lists
library(vroom)     # For reading in many files at once
library(broom)     # For tidying model output
library(fs)        # For file system operations
library(gt)        # For formatting tables
library(knitr)     # For markdown tables
library(ggrepel)   # For labelling points without overlap
library(skimr)     # For data frame level summary
library(ggridges)  # For ridge plots
library(plotly)    # For interactive plots
library(kableExtra)# For formatting tables (with grouped rows)
```

# 1. Data

Load the student scores for the test - here we load the ETH Zurich test data, downloaded from https://pontifex.ethz.ch/s21t5/rate/

```{r load-all-data, echo=FALSE, message=FALSE}
# produce list of all the relevant file names
# (match only "all.csv" in the -q36 versions)
files_pre <- dir_ls("data-eth/", recurse = TRUE, regexp = "-q36/s21t-000.*.csv")

# read all files and add a column called file_path to identify them
scores_pre <- vroom(files_pre, id = "file_path")

# parse file_path column into year and class
test_scores_pre <- scores_pre %>%
  mutate(
    file_path = str_remove(file_path, "data-eth/"),
    file_path = str_remove(file_path, "-q36"),
    file_path = str_remove(file_path, ".csv"),
    ) %>%
  separate(file_path, c("year", "class"), sep = "/")


# produce list of all the relevant file names
# (match only "all.csv" in the -q32 versions)
files_post <- dir_ls("data-eth/", recurse = TRUE, regexp = "-q32/s21t-000.*.csv")

# read all files and add a column called file_path to identify them
scores_post <- vroom(files_post, id = "file_path")

# parse file_path column into year and class
test_scores_post <- scores_post %>%
  mutate(
    file_path = str_remove(file_path, "data-eth/"),
    file_path = str_remove(file_path, "-q32"),
    file_path = str_remove(file_path, ".csv"),
    ) %>%
  separate(file_path, c("year", "class"), sep = "/") %>% 
  # relabel the columns to use B rather than A for the question names
  rename_with(~ str_replace(., "^A", "B"))

#
# Join data from both test versions together
#
# This achieves something like the planned function tidymirt::combined_test_versions
# see https://github.com/mine-cetinkaya-rundel/tidymirt/issues/1
#

# construct unique item names that show where each item appeared on each version
# e.g. A4_B7 means the item was Q4 on version A and Q7 on version B
# (A0 and B0 are used to indicate the question did not appear on that version)
test_versions <- read_csv("data-eth/eth-metadata.csv") %>% 
  mutate(
    label = case_when(
      !is.na(pre) & !is.na(post) ~ str_glue("{pre}_{post}"),
      !is.na(pre) & is.na(post) ~ str_glue("{pre}_B0"),
      is.na(pre) & !is.na(post) ~ str_glue("A0_{post}"),
    ),
    item_num = row_number()
  ) %>% 
  mutate(
    outcome = case_when(
      !is.na(pre) & !is.na(post) ~ "unchanged",
      !is.na(pre) & is.na(post) ~ "removed",
      is.na(pre) & !is.na(post) ~ "added",
    )
  )

# helper function - given a df and two character vectors of the same length, this will
# rename the columns of df, replacing values from old_names with corresponding values from new_names
replace_names <- function(df, old_names, new_names) {
  name_lookup <- bind_cols("old" = old_names, "new" = new_names) %>%
    # remove any rows with NA, since that will cause problems for rename_with
    drop_na()
  df %>%
    rename_with(.fn = ~name_lookup$new, .cols = name_lookup$old)
}

test_scores = bind_rows(
  "pre" = test_scores_pre %>%
    replace_names(old_names = test_versions$pre, new_names = test_versions$label),
  "post" = test_scores_post %>%
    replace_names(old_names = test_versions$post, new_names = test_versions$label),
  .id = "test_version"
)
```

<details>
    <summary>Show data summary</summary>
```{r data-peek}
skim(test_scores)
```
</details>

The scores are:

* 0 if the student gave an incorrect response
* 1 if the student gave a correct response
* 2 if the student chose the "I don't know" answer option
* NA if there was no response recorded

For this analysis, we replace the "2 = I don't know" scores with 0, reflecting a non-correct answer.

```{r}
test_scores <- test_scores %>% 
  mutate(across(starts_with("A"), ~ ifelse(. == 2, 0, .)))
```

## Data summary

The number of responses from each cohort:

```{r skim-classes, warning=FALSE}
test_scores %>% 
  group_by(year) %>% 
  tally() %>% 
  gt() %>% 
  data_color(
    columns = c("n"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  )
```

There is the same (roughly normal) distribution of raw scores each year:

```{r}
total_scores <- test_scores %>% mutate(total = rowSums(across(where(is.numeric)), na.rm = TRUE))

total_scores %>%
  ggplot(aes(x = total)) +
  geom_histogram(binwidth = 1) +
  facet_wrap(vars(year)) +
  theme_minimal()
```

Mean and standard deviation for each item (note this table is very wide - see the scroll bar at the bottom!):

```{r skim-all-data}
test_scores %>% 
  select(-class, -test_version) %>% 
  group_by(year) %>% 
  skim_without_charts() %>% 
  select(-contains("character."), -contains("numeric.p"), -skim_type) %>% 
  rename(complete = complete_rate) %>% 
  # make the table wider, i.e. with separate columns for each year's results, with the year at the start of each column name
  pivot_wider(names_from = year, values_from = -c(skim_variable, year), names_glue = "{year}__{.value}") %>% 
  # put the columns in order by year
  select(sort(names(.))) %>% 
  select(skim_variable, everything()) %>% 
  # use GT to make the table look nice
  gt(rowname_col = "skim_variable") %>% 
  # group the columns from each year
  tab_spanner_delim(delim = "__") %>%
  fmt_number(columns = contains("numeric"), decimals = 2) %>%
  fmt_percent(columns = contains("complete"), decimals = 0) %>% 
  # change all the numeric.mean and numeric.sd column names to Mean and SD
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.mean"), label = "Mean") %>% deframe()
  ) %>% 
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.sd"), label = "SD") %>% deframe()
  ) %>%
  data_color(
    columns = contains("numeric.mean"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  )
```

# 2. Testing assumptions

Before applying IRT, we should check that the data satisfies the assumptions needed by the model. In particular, to use a 1-dimensional IRT model, we should have some evidence of unidimensionality in the test scores.

Since the combined data on the two versions of the test have large portions of "missing" data (e.g. no responses to new questions from students who completed the old test), it is not possible to carry out the factor analysis as in the analyse-test script, since the factor analysis routine does not work with missing data.

Instead, in the next section we proceed directly to fitting the IRT model, and using the $Q_3$ check for local independence. In the final section, we also run a factor analysis for the data from the new test only.

# 3. Fitting 2 parameter logistic MIRT model

We use the `mirt` package to fit an item response theory model.

For this analysis, we use the 2 parameter logistic (2PL) model.

<details>
    <summary>Show model fitting output</summary>
```{r fit-mirt, warning=FALSE, message=FALSE}
item_scores <- test_scores %>% 
  select(-class, -year, -test_version)

fit_2pl <- mirt(
  data = item_scores, # just the columns with question scores
  #removeEmptyRows = TRUE, 
  model = 1,          # number of factors to extract
  itemtype = "2PL",   # 2 parameter logistic model
  SE = TRUE           # estimate standard errors
  )
```
</details>

## Local independence

We compute Yen's $Q_3$ (1984) to check for any dependence between items after controlling for $\theta$. This gives a score for each pair of items, with scores above 0.2 regarded as problematic (see DeMars, p. 48).

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
residuals_2pl <- residuals(fit_2pl, type = "Q3") %>% data.frame
```
```{r}
residuals_2pl  %>% as.matrix() %>% 
  corrplot::corrplot(type = "upper")
```

This shows that most item pairs are independent, with only a couple of pairs showing cause for concern:

```{r}
residuals_2pl %>%
  rownames_to_column(var = "item1") %>%
  as_tibble() %>% 
  pivot_longer(cols = starts_with("A"), names_to = "item2", values_to = "Q3_score") %>% 
  filter(abs(Q3_score) > 0.2) %>% 
  filter(parse_number(item1) < parse_number(item2)) %>%
  gt()
```

In fact, both of these pairs highlight questions that were removed from the test:

* A18 and A19 are on the product and quotient rules, and only A18 was retained on the new test,

* A34 and A35 are both about vectors; only A34 was retained (in modified form, as B30)

Given that this violation of the local independence assumption is very mild, we proceed using this model.

## Model parameters
We then compute factor score estimates and augment the existing data frame with these estimates, to keep everything in one place. To do the estimation, we use the `fscores()` function from the mirt package which takes in a computed model object and computes factor score estimates according to the method specified. We will use the EAP method for factor score estimation, which is the "expected a-posteriori" method, the default.

```{r augment-with-f1}
test_scores <- test_scores %>%
  mutate(F1 = fscores(fit_2pl, method = "EAP"))
```

We can also calculate the model coefficient estimates using a generic function `coef()` which is used to extract model coefficients from objects returned by modeling functions. We will set the `IRTpars` argument to `TRUE`, which means slope intercept parameters will be converted into traditional IRT parameters.

```{r extract-coefs}
coefs_2pl <- coef(fit_2pl, IRTpars = TRUE)
```

The resulting object `coefs` is a list, with one element for each question, and an additional `GroupPars` element that we won't be using. For each question, the object records several values:

* `a` is discrimination
* `b` is difficulty
* endpoints of the 95% confidence intervals are also shown

To make this output a little more user friendly, we use the `tidy_mirt_coefs` function that we have provided in `common-functions.R`, to produce a single data frame with a row for each question.

```{r map-tidy-mirt-coefs}
source("common-functions.R")
# use head(., -1) to remove the last element, `GroupPars`, which does not correspond to a question
tidy_2pl <- map_dfr(head(coefs_2pl, -1), tidy_mirt_coefs, .id = "Question") %>% 
  left_join(test_versions, by = c("Question" = "label"))
```

Here is a nicely formatted table of the result:

```{r tabulate-tidy-output}
tidy_2pl %>%
  select(-pre,-post,-notes) %>% 
  group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("a_"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("b_"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  ) %>%
  tab_spanner(label = "Discrimination", columns = contains("a_")) %>%
  tab_spanner(label = "Difficulty", columns = contains("b_")) %>%
  cols_label(
    a_est = "Est.",
    b_est = "Est.",
    a_CI_2.5 = "2.5%",
    b_CI_2.5 = "2.5%",
    a_CI_97.5 = "97.5%",
    b_CI_97.5 = "97.5%"
  )
```

These values are also saved to the file `output/eth_post_2pl-results.csv`.

```{r save-2pl-results}
tidy_2pl %>% 
  write_csv("output/eth_post_2pl-results.csv")
```


```{r wright-map, eval=FALSE, include=FALSE}
# Basic version of the wright map
tidy_2pl %>% 
  mutate(qnum = parse_number(Question)) %>% 
  ggplot(aes(x = qnum, y = b_est, label = Question)) +
  geom_errorbar(aes(ymin = b_CI_2.5, ymax = b_CI_97.5), width = 0.2) +
  geom_text(colour = "grey") +
  geom_point() +
  theme_minimal() +
  labs(x = "Question",
       y = "Difficulty")
```

This shows the difficulty and discrimination of each of the questions, highlighting those that were added or removed:

```{r}
tidy_2pl %>% 
  mutate(qnum = parse_number(Question)) %>% 
  ggplot(aes(
    x = a_est,
    y = b_est,
    label = case_when(
      outcome == "unchanged" ~ "",
      outcome == "removed" ~ pre,
      outcome == "added" ~ post
    ),
    colour = outcome
  )) +
  geom_errorbar(aes(ymin = b_CI_2.5, ymax = b_CI_97.5), width = 0, alpha = 0.5) +
  geom_errorbar(aes(xmin = a_CI_2.5, xmax = a_CI_97.5), width = 0, alpha = 0.5) +
  geom_text_repel() +
  geom_point() +
  theme_minimal() +
  labs(x = "Discrimination",
       y = "Difficulty")
```


## Comparing years and classes

Do students from different programmes of study have different distributions of ability?

### Differences between years

Compare the distribution of abilities in the year groups (though in this case there is only one).

```{r viz-years}
ggplot(test_scores, aes(F1, y = year, fill = as.factor(year), colour = as.factor(year))) +
  geom_density_ridges(alpha=0.5) + 
  scale_x_continuous(limits = c(-3.5,3.5)) +
  labs(title = "Density plot", 
       subtitle = "Ability grouped by year of taking the test", 
       x = "Ability", y = "Density",
       fill = "Year", colour = "Year") +
  theme_minimal()
```

## Information curves

### Test information curve

```{r}
plot(fit_2pl, type = "infoSE", main = "Test information")
```

### Item information curves

```{r}
item_info_plots <- plot(fit_2pl, type = "infotrace", main = "Item information curves")
item_info_plots
```

Using the data from these plots, we can compute the sums of different subsets of the information curves, to help evaluate the changes made to the test.

```{r fig.height=3, fig.width=5}
# Extract the data from the plot, so it can be re-plotted in new ways
item_info_data <- tibble(item = names(item_info_plots$packet.sizes),
                         item_data = item_info_plots$panel.args) %>%
  unnest_wider(item_data) %>%
  unnest(c(x, y)) %>% 
  left_join(test_versions %>% select(item = label, outcome), by = "item")

item_info_data %>% 
  group_by(x) %>% 
  summarise(
    #items_all = sum(y),
    items_pre = sum(ifelse(outcome %in% c("unchanged", "removed"), y, 0)),
    items_post = sum(ifelse(outcome %in% c("unchanged", "added"), y, 0)),
    items_added = sum(ifelse(outcome %in% c("added"), y, 0)),
    items_removed = sum(ifelse(outcome %in% c("removed"), y, 0))
  ) %>% 
  pivot_longer(cols = starts_with("items_"), names_to = "items", names_prefix = "items_", values_to = "y") %>% 
  mutate(panel = ifelse(items %in% c("pre", "post"), "overall", "changes") %>% fct_rev()) %>% 
  mutate(items = fct_relevel(items, "removed", "added", "pre", "post")) %>% 
  ggplot(aes(x = x, y = y, colour = items)) +
  geom_line() +
  scale_colour_brewer(palette = "Paired") +
  facet_wrap(vars(panel), scales = "free_y") +
  labs(x = "Ability", y = "Information") +
  theme_minimal()

ggsave("output/eth_compare_info-curves.pdf", units = "cm", width = 14, height = 6)
```

This shows that the overall distribution is quite stable between the two versions of the test. The changes have reduced the information, but this was to be expected as the number of items was reduced.

Looking more closely at the information curves for the items that were added and removed, and noting that the number of items in each case is different, we consider instead the mean information per item:

```{r fig.height=3, fig.width=4}
item_info_data %>% 
  group_by(x) %>% 
  summarise(
    n_added = sum(ifelse(outcome %in% c("added"), 1, 0)),
    items_added = sum(ifelse(outcome %in% c("added"), y, 0)) / n_added,
    items_removed = sum(ifelse(outcome %in% c("removed"), y, 0)) / sum(ifelse(outcome %in% c("removed"), 1, 0))
  ) %>% 
  pivot_longer(cols = starts_with("items_"), names_to = "items", names_prefix = "items_", values_to = "y") %>% 
  mutate(items = fct_relevel(items, "removed", "added")) %>% 
  ggplot(aes(x = x, y = y, colour = items)) +
  geom_line() +
  scale_colour_brewer(palette = "Paired") +
  labs(x = "Ability", y = "Mean information per item") +
  theme_minimal()

ggsave("output/eth_compare_mean-item-info.pdf", units = "cm", width = 10, height = 6)
```

This shows that the questions that added questions offer, on average, slightly more information than the questions they replaced (however there are fewer of them, hence lower total information as shown above). The difference is largest in the ability range 0-2.

## Total information

Using `mirt`'s `areainfo` function, we can find the total area under the information curves.

```{r}
all_items = test_versions %>% filter(!is.na(pre) | !is.na(post)) %>% pull(item_num) # amend the filter to consider different subsets of items
info_2pl <- areainfo(fit_2pl, c(-4,4), which.items = all_items)
info_2pl %>% gt()
```

This shows that the total information in all items (from both versions A and B of the test) is `r info_2pl$TotalInfo`.

### Information by item

```{r}
tidy_info <- test_versions %>%
  mutate(TotalInfo = purrr::map_dbl(
    item_num,
    ~ areainfo(fit_2pl,
               c(-4, 4),
               which.items = .x) %>% pull(TotalInfo)
  ))

tidy_info %>%
  select(label, TotalInfo, description, outcome) %>% 
  arrange(-TotalInfo) %>% 
  #group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("info"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("outcome"),
    colors = scales::col_factor(palette = c("viridis"), domain = NULL)
  ) %>%
  cols_label(
    TotalInfo = "Information"
  )
```


### Groups of questions

Grouping the questions into those that were unchanged, removed and added between the two versions:

```{r}
versions_info <- purrr::map_dfr(
  c("unchanged", "removed", "added"),
  ~ areainfo(
    fit_2pl,
    c(-4, 4),
    which.items = test_versions %>% filter(outcome == .x) %>% pull(item_num)
  ) %>% mutate(version = .x, .before = 1)
) %>%
  select(-LowerBound,-UpperBound,-Info,-Proportion) %>%
  mutate(mean = TotalInfo / nitems)

versions_info %>%
  mutate(version = paste(str_to_title(version), "questions")) %>% 
  gt() %>%
  cols_label(
    version = "Subset",
    TotalInfo = "Total information",
    nitems = "Number of items",
    mean = "Mean information per item"
  )
```

### Mean information

Thus, while the changes to the test have not increased the overall information in the test (and in fact have slightly decreased it due to there being fewer questions), they _have_ increased the mean information per question on the test:

```{r}
info_old <- areainfo(fit_2pl,
                     c(-4, 4),
                     which.items = test_versions %>% filter(outcome %in% c("unchanged", "removed")) %>% pull(item_num))
info_new <- areainfo(fit_2pl,
                     c(-4, 4),
                     which.items = test_versions %>% filter(outcome %in% c("unchanged", "added")) %>% pull(item_num))

versions_info <- bind_rows("Version A" = info_old,
                           "Version B" = info_new,
                           .id = "version") %>% 
  select(-LowerBound, -UpperBound, -Info, -Proportion) %>% 
  mutate(mean = TotalInfo / nitems)

versions_info %>% gt() %>%
  cols_label(
    version = "Test version",
    TotalInfo = "Total information",
    nitems = "Number of items",
    mean = "Mean information per item"
  )
```




## Response curves

### Test response curves

```{r}
plot(fit_2pl, type = "score", auto.key = FALSE)
```

### Item response curves

We can get individual item surface and information plots using the `itemplot()` function from the **mirt** package, e.g.

```{r}
mirt::itemplot(fit_2pl, item = 1, 
               main = "Trace lines for item 1")
```

We can also get the plots for all trace lines, one facet per plot.

```{r}
plot(fit_2pl, type = "trace", auto.key = FALSE)
```

Or all of them overlaid in one plot.

```{r}
plot(fit_2pl, type = "trace", facet_items=FALSE)
```

An alternative approach is using ggplot2 and plotly to add interactivity to make it easier to identify items.

```{r warning=FALSE}
# store the object
plt <- plot(fit_2pl, type = "trace", facet_items = FALSE)
# the data we need is in panel.args
# TODO - I had to change the [[1]] to [[2]] since the plt has two panels for some reason, with the one we want being the 2nd panel
plt_data <- tibble(
  x          = plt$panel.args[[2]]$x,
  y          = plt$panel.args[[2]]$y,
  subscripts = plt$panel.args[[2]]$subscripts,
  item       = rep(colnames(item_scores), each = 200)
) %>%
  mutate(
    item_no = str_remove(item, "A") %>% as.numeric(),
    item    = fct_reorder(item, item_no)
    )

head(plt_data)

plt_gg <- ggplot(plt_data, aes(x, y, 
                          colour = item, 
                          text = item)) + 
  geom_line() + 
  labs(
    title = "2PL - Trace lines",
    #x = expression(theta),
    x = "theta",
    #y = expression(P(theta)),
    y = "P(theta)",
    colour = "Item"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

ggplotly(plt_gg, tooltip = "text")
```



# 4. Factor analysis for the new test only {.tabset}

## Factor analysis setup

Here we redo the factor analysis, but using only the data from the new version of the test.

```{r}
item_scores_B <- test_scores %>% 
  select(-F1) %>% 
  select(-contains("B0")) %>% 
  filter(test_version == "post") %>% 
  select(-class, -year, -test_version)
```

The `parameters` package provides functions that run various checks to see if the data is suitable for factor analysis, and if so, how many factors should be retained.

```{r fa-checks-B}
structure <- check_factorstructure(item_scores_B)
n <- n_factors(item_scores_B)
```

```{r fa-structure-check-B, echo=FALSE, results="asis"}
# check_factorstructure(item_scores)

# HACK - to make the heading printed by easystats be h4 rather than h1, use ### 
# TODO - perhaps suggest modification to https://github.com/easystats/insight/blob/master/R/print.easystats_check.R and https://github.com/easystats/parameters/blob/cbbe89c469148735110d5c16ef153e72a20bb0a0/R/n_factors.R#L352

res <- capture.output(structure)
cat(paste0("###", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors-B, echo=FALSE, results = "asis"}
res <- capture.output(n)
cat(paste0("###", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors-details-B}
plot(n)
summary(n) %>% gt()
#n %>% tibble() %>% gt()
```

The scree plot shows the eignvalues associated with each factor:

```{r fa-scree-B}
fa.parallel(item_scores_B, fa = "fa")
```

Based on this, there is clear support for a 1-factor solution. We also consider the 4-factor solution.

## 1 Factor

<details>
    <summary>Show factanal output</summary>
```{r fa1-B}
fitfact <- factanal(item_scores_B, factors = 1, rotation = "varimax")
print(fitfact, digits = 2, cutoff = 0.3, sort = TRUE)

load <- tidy(fitfact)
```
</details>

```{r warning=FALSE}
ggplot(load, aes(x = fl1, y = 0)) + 
  geom_point() + 
  geom_label_repel(aes(label = paste0("A", rownames(load))), show.legend = FALSE) +
  labs(x = "Factor 1", y = NULL,
       title = "Standardised Loadings", 
       subtitle = "Based upon correlation matrix") +
  theme_minimal()
```


```{r}
load %>% 
  select(question = variable, factor_loading = fl1) %>% 
  left_join(test_versions %>% select(question = label, description), by = "question") %>% 
  arrange(-factor_loading) %>% 
  gt() %>%
  data_color(
    columns = contains("factor"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  )
```

The questions that load most strongly on this factor are very standard calculations in integration, differentiation, and trigonometry -- which is consistent with the aim of the test.

> **TODO** - add comment saying it is disappointing that the new questions are all at the low end of this table?

## 4 Factor

Here we also investigate the 4-factor solution, to see whether these factors are interpretable.

<details>
    <summary>Show factanal output</summary>
```{r fa4-B}
fitfact4 <- factanal(item_scores_B, factors = 4, rotation = "varimax")
print(fitfact4, digits = 2, cutoff = 0.3, sort = TRUE)

load4 <- tidy(fitfact4)
```
</details>

```{r}
ggplot(load4, aes(x = fl1, y = fl2)) + 
  geom_point() + 
  geom_label_repel(aes(label = paste0("A", rownames(load))), show.legend = FALSE) +
  labs(x = "Factor 1", y = "Factor 2",
       title = "Standardised Loadings", 
       subtitle = "Based upon correlation matrix") +
  theme_minimal()
```

```{r message=FALSE, warning=FALSE}
main_factors <- load4 %>% 
#  mutate(factorNone = 0.4) %>%  # add this to set the main factor to "None" where all loadings are below 0.4
  pivot_longer(names_to = "factor",
               cols = contains("fl")) %>% 
  mutate(value_abs = abs(value)) %>% 
  group_by(variable) %>% 
  top_n(1, value_abs) %>% 
  ungroup() %>% 
  transmute(main_factor = factor, variable)

load4 %>% 
  select(-uniqueness) %>% 
  # add the info about which is the main factor
  left_join(main_factors, by = "variable") %>%
  left_join(test_versions %>% select(variable = label, description), by = "variable") %>% 
  arrange(main_factor) %>% 
  select(main_factor, everything()) %>% 
  # arrange adjectives by descending loading on main factor
  rowwise() %>% 
  mutate(max_loading = max(abs(c_across(starts_with("fl"))))) %>% 
  group_by(main_factor) %>% 
  arrange(-max_loading, .by_group = TRUE) %>% 
  select(-max_loading) %>% 
  # sort out the presentation
  rename("Main Factor" = main_factor, # the _ throws a latex error
         "Question" = variable) %>%
  mutate_at(
    vars(starts_with("fl")),
    ~ cell_spec(round(., digits = 3), bold = if_else(abs(.) > 0.4, T, F))
  ) %>% 
  kable(booktabs = T, escape = F, longtable = T) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>%
  kableExtra::kable_styling(latex_options = c("repeat_header"))
```

**TODO** add comments from Meike here

> The first factor is again calculations, but this time only in calculus (i.e. integrals and derivatives).
> 
> The second factor seems to be something like "abstract stuff", it has to do with limits, rules for logarithms etc.
> 
> I guess that could be a category of its own.
> 
> The third one is interesting. It's clearly graphical interpretations. All in different settings, but clearly belonging together.
> 
> And of the fourth factor I cannot make sense


# About this report

This report supports the analysis in the following paper:

> [citation needed]

## Packages

In this analysis we used the following packages. You can learn more about each one by clicking on the links below.

- [**mirt**](https://cran.r-project.org/web/packages/mirt/mirt.pdf): For IRT analysis
- [**psych**](https://personality-project.org/r/psych/): For factor analysis
- [**tidyverse**](https://tidyverse.org/): For data wrangling and visualisation
- [**reshape**](http://had.co.nz/reshape/): For reshaping nested lists
- [**vroom**](https://vroom.r-lib.org/): For reading in many files at once
- [**broom**](https://broom.tidymodels.org/): For tidying model output
- [**fs**](https://fs.r-lib.org/): For file system operations
- [**gt**](https://gt.rstudio.com/): For formatting tables
- [**knitr**](https://yihui.org/knitr/): For markdown tables
- [**ggrepel**](https://ggrepel.slowkow.com/): For labelling points without overlap
- [**skimr**](https://docs.ropensci.org/skimr/): For data frame level summary
- [**ggridges**](https://wilkelab.org/ggridges/): For ridge plots


