---
title: "Compare two versions of a multiple-choice test (ETH Zurich)"
author: "George Kinnear"
date: '2021-10-05'
output:
  html_document:
    toc: yes
    toc_float: yes
editor_options:
  chunk_output_type: console
---

```{r load-packages, message=FALSE, include=FALSE}
library(mirt)      # For IRT analysis
library(psych)     # For factor analysis
library(parameters)# For factor analysis
library(tidyverse) # For data wrangling and visualisation
library(reshape)   # For reshaping nested lists
library(vroom)     # For reading in many files at once
library(broom)     # For tidying model output
library(fs)        # For file system operations
library(gt)        # For formatting tables
library(knitr)     # For markdown tables
library(ggrepel)   # For labelling points without overlap
library(skimr)     # For data frame level summary
library(ggridges)  # For ridge plots
library(plotly)    # For interactive plots
```

# 1. Data

Load the student scores for the test - here we load the ETH Zurich test data, downloaded from https://pontifex.ethz.ch/s21t5/rate/

```{r load-all-data, echo=FALSE, message=FALSE}
# produce list of all the relevant file names
# (match only "all.csv" in the -q36 versions)
files_pre <- dir_ls("data-eth/", recurse = TRUE, regexp = "-q36/s21t-000.*.csv")

# read all files and add a column called file_path to identify them
scores_pre <- vroom(files_pre, id = "file_path")

# parse file_path column into year and class
test_scores_pre <- scores_pre %>%
  mutate(
    file_path = str_remove(file_path, "data-eth/"),
    file_path = str_remove(file_path, "-q36"),
    file_path = str_remove(file_path, ".csv"),
    ) %>%
  separate(file_path, c("year", "class"), sep = "/")


# produce list of all the relevant file names
# (match only "all.csv" in the -q32 versions)
files_post <- dir_ls("data-eth/", recurse = TRUE, regexp = "-q32/s21t-000.*.csv")

# read all files and add a column called file_path to identify them
scores_post <- vroom(files_post, id = "file_path")

# parse file_path column into year and class
test_scores_post <- scores_post %>%
  mutate(
    file_path = str_remove(file_path, "data-eth/"),
    file_path = str_remove(file_path, "-q32"),
    file_path = str_remove(file_path, ".csv"),
    ) %>%
  separate(file_path, c("year", "class"), sep = "/") %>% 
  # relabel the columns to use B rather than A for the question names
  rename_with(~ str_replace(., "^A", "B"))

#
# Join data from both test versions together
#
# This achieves something like the planned function tidymirt::combined_test_versions
# see https://github.com/mine-cetinkaya-rundel/tidymirt/issues/1
#

# construct unique item names that show where each item appeared on each version
# e.g. A4_B7 means the item was Q4 on version A and Q7 on version B
# (A0 and B0 are used to indicate the question did not appear on that version)
test_versions <- read_csv("data-eth/eth-metadata.csv") %>% 
  mutate(
    label = case_when(
      !is.na(pre) & !is.na(post) ~ str_glue("{pre}_{post}"),
      !is.na(pre) & is.na(post) ~ str_glue("{pre}_B0"),
      is.na(pre) & !is.na(post) ~ str_glue("A0_{post}"),
    )
  ) %>% 
  mutate(
    outcome = case_when(
      !is.na(pre) & !is.na(post) ~ "unchanged",
      !is.na(pre) & is.na(post) ~ "removed",
      is.na(pre) & !is.na(post) ~ "added",
    )
  )

# helper function - given a df and two character vectors of the same length, this will
# rename the columns of df, replacing values from old_names with corresponding values from new_names
replace_names <- function(df, old_names, new_names) {
  name_lookup <- bind_cols("old" = old_names, "new" = new_names) %>%
    # remove any rows with NA, since that will cause problems for rename_with
    drop_na()
  df %>%
    rename_with(.fn = ~name_lookup$new, .cols = name_lookup$old)
}

test_scores = bind_rows(
  "pre" = test_scores_pre %>%
    replace_names(old_names = test_versions$pre, new_names = test_versions$label),
  "post" = test_scores_post %>%
    replace_names(old_names = test_versions$post, new_names = test_versions$label),
  .id = "test_version"
)

# The data from ETH Zurich includes a score of 2 if the student selected "I don't know"
# Here we replace those scores with NA, to reflect essentially missing data
# TODO - consider mapping this to 0 instead, as it reflects being "not correct"
test_scores <- test_scores %>% 
  mutate(across(starts_with("A"), ~ na_if(., 2))) %>%
  # A few students (23 out of 9671) gave "I don't know" as their answer to every question
  # Here we remove them, since rows with all NAs cause problems for mirt
  filter(!if_all(starts_with("A"), is.na))
```

```{r data-peek}
head(test_scores_pre)
head(test_scores_post)
```

## Data summary

The number of responses from each cohort:

```{r skim-classes}
test_scores %>% 
  group_by(year) %>% 
  tally() %>% 
  gt() %>% 
  data_color(
    columns = c("n"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  )
```

Mean and standard deviation for each item:

```{r skim-all-data}
test_scores %>% 
  select(-class, -test_version) %>% 
  group_by(year) %>% 
  skim_without_charts() %>% 
  select(-contains("character."), -contains("numeric.p"), -skim_type) %>% 
  rename(complete = complete_rate) %>% 
  # make the table wider, i.e. with separate columns for each year's results, with the year at the start of each column name
  pivot_wider(names_from = year, values_from = -c(skim_variable, year), names_glue = "{year}__{.value}") %>% 
  # put the columns in order by year
  select(sort(names(.))) %>% 
  select(skim_variable, everything()) %>% 
  # use GT to make the table look nice
  gt(rowname_col = "skim_variable") %>% 
  # group the columns from each year
  tab_spanner_delim(delim = "__") %>%
  fmt_number(columns = contains("numeric"), decimals = 2) %>%
  fmt_percent(columns = contains("complete"), decimals = 0) %>% 
  # change all the numeric.mean and numeric.sd column names to Mean and SD
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.mean"), label = "Mean") %>% deframe()
  ) %>% 
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.sd"), label = "SD") %>% deframe()
  ) %>%
  data_color(
    columns = contains("numeric.mean"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  )
```

# 2. Testing assumptions

Before applying IRT, we should check that the data satisfies the assumptions needed by the model. In particular, to use a 1-dimensional IRT model, we should have some evidence of unidimensionality in the test scores.

### Local independence

This plot shows the correlations between scores on each pair of items -- note that it is restricted to only those items that appear on both versions of the test, since the plotting package did not deal well with missing data:

```{r corr-plot}
item_scores <- test_scores %>% 
  select(-class, -year, -test_version)

item_scores_unchanged_only <- item_scores %>% 
  select(!contains("B0")) %>% select(!contains("A0"))

cor_ci <- psych::corCi(item_scores_unchanged_only, plot = FALSE)

psych::cor.plot.upperLowerCi(cor_ci)
```

There are a few correlations that are not significantly different from 0:

```{r cor-not-corr}
cor_ci$ci %>% 
  as_tibble(rownames = "corr") %>% 
  filter(p > 0.05) %>% 
  arrange(-p) %>% 
  select(-contains(".e")) %>% 
  gt() %>% 
  fmt_number(columns = 2:4, decimals = 3)
```

Here we redo the correlation calculations with all the items, and check that there are still few cases where the correlations close to 0:

```{r}
cor_ci2 <- psych::corCi(item_scores, plot = FALSE)
cor_ci2$ci %>% 
  as_tibble(rownames = "corr") %>% 
  filter(p > 0.05) %>% 
  arrange(-p) %>% 
  select(-contains(".e")) %>% 
  gt() %>% 
  fmt_number(columns = 2:4, decimals = 3)
```


The overall picture is that the item scores are well correlated with each other.

### Dimensionality

Here we again focus on the subset of items that appeared in both tests.

```{r fa-checks}
structure <- check_factorstructure(item_scores_unchanged_only)
n <- n_factors(item_scores_unchanged_only)
```

```{r fa-structure-check, echo=FALSE, results="asis"}
# check_factorstructure(item_scores)

# HACK - to make the heading printed by easystats be h4 rather than h1, use ### 
# TODO - perhaps suggest modification to https://github.com/easystats/insight/blob/master/R/print.easystats_check.R and https://github.com/easystats/parameters/blob/cbbe89c469148735110d5c16ef153e72a20bb0a0/R/n_factors.R#L352

res <- capture.output(structure)
cat(paste0("###", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors, echo=FALSE, results = "asis"}
res <- capture.output(n)
cat(paste0("###", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors-details}
plot(n)
summary(n) %>% gt()
#n %>% tibble() %>% gt()
```


```{r fa-scree}
fa.parallel(item_scores_unchanged_only, fa = "fa")
```

### 1 Factor

```{r fa1}
item_scores_unchanged_only_and_no_na <- item_scores_unchanged_only %>%
  mutate(
    across(everything(), ~replace_na(.x, 0))
  )
fitfact <- factanal(item_scores_unchanged_only_and_no_na, factors = 1, rotation = "varimax")
print(fitfact, digits = 2, cutoff = 0.3, sort = TRUE)

load <- tidy(fitfact)

ggplot(load, aes(x = fl1, y = 0)) + 
  geom_point() + 
  geom_label_repel(aes(label = paste0("A", rownames(load))), show.legend = FALSE) +
  labs(x = "Factor 1", y = NULL,
       title = "Standardised Loadings", 
       subtitle = "Based upon correlation matrix") +
  theme_minimal()
```


```{r}
# To proceed with the IRT analysis, comment out the following line before knitting
#knitr::knit_exit()
```


# 3. Fitting 2 parameter logistic MIRT model

We can fit a Multidimensional Item Response Theory (mirt) model. From the function definition:

```
mirt fits a maximum likelihood (or maximum a posteriori) factor analysis model to any mixture of dichotomous and polytomous data under the item response theory paradigm using either Cai's (2010) Metropolis-Hastings Robbins-Monro (MHRM) algorithm.
```

The process is to first fit the model, and save the result as a model object that we can then parse to get tabular or visual displays of the model that we might want. When fitting the model, we have the option to specify a few arguments, which then get interpreted as parameters to be passed to the model.

```{r fit-mirt, warning=FALSE, message=FALSE}
fit_2pl <- mirt(
  data = item_scores, # just the columns with question scores
  #removeEmptyRows = TRUE, 
  model = 1,          # number of factors to extract
  itemtype = "2PL",   # 2 parameter logistic model
  SE = TRUE           # estimate standard errors
  )
```

We then compute factor score estimates and augment the existing data frame with these estimates, to keep everything in one place. To do the estimation, we use the `fscores()` function from the mirt package which takes in a computed model object and computes factor score estimates according to the method specified. We will use the EAP method for factor score estimation, which is the "expected a-posteriori" method, the default. We specify it explicitly below, but the results would have been the same if we omitted specifying the method argument since it's the default method the function uses.

```{r augment-with-f1}
test_scores <- test_scores %>%
  mutate(F1 = fscores(fit_2pl, method = "EAP"))
```

We can also calculate the model coefficient estimates using a generic function `coef()` which is used to extract model coefficients from objects returned by modeling functions. We will set the `IRTpars` argument to `TRUE`, which means slope intercept parameters will be converted into traditional IRT parameters.

```{r extract-coefs}
coefs_2pl <- coef(fit_2pl, IRTpars = TRUE)
```

The resulting object `coefs` is a list, with one element for each question, and an additional `GroupPars` element that we won't be using. The output is a bit long, so we're only showing a few of the elements here:

```{r peek-coefs}
coefs_2pl[1:3]
# coefs_2pl[35:37]
```

Let's take a closer look at the first element:

```{r coef-1}
coefs_2pl[1]
```

In this output:

* `a` is discrimination
* `b` is difficulty
* endpoints of the 95% confidence intervals are also shown

To make this output a little more user friendly, we should tidy it such that we have a row per question. We'll do this in two steps. First, write a function that tidies the output for one question, i.e. one list element. Then, map this function over the list of all questions, resulting in a data frame.

```{r f-tidy-mirt-coefs}
tidy_mirt_coefs <- function(x){
  x %>%
    # melt the list element
    melt() %>%
    # convert to a tibble
    as_tibble() %>%
    # convert factors to characters
    mutate(across(where(is.factor), as.character)) %>%
    # only focus on rows where X2 is a or b (discrimination or difficulty)
    filter(X2 %in% c("a", "b")) %>%
    # in X1, relabel par (parameter) as est (estimate)
    mutate(X1 = if_else(X1 == "par", "est", X1)) %>%
    # unite columns X2 and X1 into a new column called var separated by _
    unite(X2, X1, col = "var", sep = "_") %>%
    # turn into a wider data frame
    pivot_wider(names_from = var, values_from = value)
}
```

Let's see what this does to a single element in `coefs`:

```{r apply-once-tidy-mirt-coefs}
tidy_mirt_coefs(coefs_2pl[1])
```

And now let's map it over all elements of coefs:

```{r map-tidy-mirt-coefs}
# use head(., -1) to remove the last element, `GroupPars`, which does not correspond to a question
tidy_2pl <- map_dfr(head(coefs_2pl, -1), tidy_mirt_coefs, .id = "Question") %>% 
  left_join(test_versions, by = c("Question" = "label"))
```

A quick peek at the result:

```{r peek-tidy-output}
tidy_2pl
```

And a nicely formatted table of the result:

```{r tabulate-tidy-output}
tidy_2pl %>%
  select(-pre,-post,-notes) %>% 
  group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("a_"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("b_"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  ) %>%
  tab_spanner(label = "Discrimination", columns = contains("a_")) %>%
  tab_spanner(label = "Difficulty", columns = contains("b_")) %>%
  cols_label(
    a_est = "Est.",
    b_est = "Est.",
    a_CI_2.5 = "2.5%",
    b_CI_2.5 = "2.5%",
    a_CI_97.5 = "97.5%",
    b_CI_97.5 = "97.5%"
  )
```

```{r wright-map}
tidy_2pl %>% 
  mutate(qnum = parse_number(Question)) %>% 
  ggplot(aes(x = qnum, y = b_est, label = Question)) +
  geom_errorbar(aes(ymin = b_CI_2.5, ymax = b_CI_97.5), width = 0.2) +
  geom_text(colour = "grey") +
  geom_point() +
  theme_minimal() +
  labs(x = "Question",
       y = "Difficulty")
```

This shows the difficulty and discrimination of each of the questions, highlighting those that were added or removed:

```{r}
tidy_2pl %>% 
  mutate(qnum = parse_number(Question)) %>% 
  ggplot(aes(
    x = a_est,
    y = b_est,
    label = case_when(
      outcome == "unchanged" ~ "",
      outcome == "removed" ~ pre,
      outcome == "added" ~ post
    ),
    colour = outcome
  )) +
  geom_errorbar(aes(ymin = b_CI_2.5, ymax = b_CI_97.5), width = 0, alpha = 0.5) +
  geom_errorbar(aes(xmin = a_CI_2.5, xmax = a_CI_97.5), width = 0, alpha = 0.5) +
  geom_text_repel() +
  geom_point() +
  theme_minimal() +
  labs(x = "Discrimination",
       y = "Difficulty")
```


## Comparing years and classes

Do students from different programmes of study have different distributions of ability?

### Differences between years

Compare the distribution of abilities in the year groups (though in this case there is only one).

```{r viz-years}
ggplot(test_scores, aes(F1, y = year, fill = as.factor(year), colour = as.factor(year))) +
  geom_density_ridges(alpha=0.5) + 
  scale_x_continuous(limits = c(-3.5,3.5)) +
  labs(title = "Density plot", 
       subtitle = "Ability grouped by year of taking the test", 
       x = "Ability", y = "Density",
       fill = "Year", colour = "Year") +
  theme_minimal()
```

## Information curves

### Test information curve

```{r}
plot(fit_2pl, type = "infoSE", main = "Test information")
```

### Item information curves

```{r}
plot(fit_2pl, type = "infotrace", main = "Item information curves")
```

## Response curves

### Test response curves

```{r}
plot(fit_2pl, type = "score", auto.key = FALSE)
```

### Item response curves

We can get individual item surface and information plots using the `itemplot()` function from the **mirt** package, e.g.

```{r}
mirt::itemplot(fit_2pl, item = 1, 
               main = "Trace lines for item 1")
```

We can also get the plots for all trace lines, one facet per plot.

```{r}
plot(fit_2pl, type = "trace", auto.key = FALSE)
```

Or all of them overlaid in one plot.

```{r}
plot(fit_2pl, type = "trace", facet_items=FALSE)
```

An alternative approach is using ggplot2 and plotly to add interactivity to make it easier to identify items.

```{r warning=FALSE}
# store the object
plt <- plot(fit_2pl, type = "trace", facet_items = FALSE)
# the data we need is in panel.args
# TODO - I had to change the [[1]] to [[2]] since the plt has two panels for some reason, with the one we want being the 2nd panel
plt_data <- tibble(
  x          = plt$panel.args[[2]]$x,
  y          = plt$panel.args[[2]]$y,
  subscripts = plt$panel.args[[2]]$subscripts,
  item       = rep(colnames(item_scores), each = 200)
) %>%
  mutate(
    item_no = str_remove(item, "A") %>% as.numeric(),
    item    = fct_reorder(item, item_no)
    )

head(plt_data)

plt_gg <- ggplot(plt_data, aes(x, y, 
                          colour = item, 
                          text = item)) + 
  geom_line() + 
  labs(
    title = "2PL - Trace lines",
    #x = expression(theta),
    x = "theta",
    #y = expression(P(theta)),
    y = "P(theta)",
    colour = "Item"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

ggplotly(plt_gg, tooltip = "text")
```


```{r}
knitr::knit_exit()
```







## Packages

In this analysis we used the following packages. You can learn more about each one by clicking on the links below.

- [**mirt**](https://cran.r-project.org/web/packages/mirt/mirt.pdf): For IRT analysis
- [**psych**](https://personality-project.org/r/psych/): For factor analysis
- [**tidyverse**](https://tidyverse.org/): For data wrangling and visualisation
- [**reshape**](http://had.co.nz/reshape/): For reshaping nested lists
- [**vroom**](https://vroom.r-lib.org/): For reading in many files at once
- [**broom**](https://broom.tidymodels.org/): For tidying model output
- [**fs**](https://fs.r-lib.org/): For file system operations
- [**gt**](https://gt.rstudio.com/): For formatting tables
- [**knitr**](https://yihui.org/knitr/): For markdown tables
- [**ggrepel**](https://ggrepel.slowkow.com/): For labelling points without overlap
- [**skimr**](https://docs.ropensci.org/skimr/): For data frame level summary
- [**ggridges**](https://wilkelab.org/ggridges/): For ridge plots


