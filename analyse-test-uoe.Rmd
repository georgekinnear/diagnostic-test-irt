---
title: "Analysis of test with partial credit (University of Edinburgh)"
author: "George Kinnear"
date: '2022-06-13'
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    number_sections: true
editor_options:
  chunk_output_type: console
---

```{r load-packages, message=FALSE, include=FALSE}
library(mirt)      # For IRT analysis
library(psych)     # For factor analysis
library(parameters)# For factor analysis
library(tidyverse) # For data wrangling and visualisation
library(reshape)   # For reshaping nested lists
library(vroom)     # For reading in many files at once
library(broom)     # For tidying model output
library(fs)        # For file system operations
library(gt)        # For formatting tables
library(kableExtra)# For formatting tables
library(knitr)     # For markdown tables
library(ggrepel)   # For labelling points without overlap
library(skimr)     # For data frame level summary
library(ggridges)  # For ridge plots
library(plotly)    # For interactive plots
library(patchwork) # For combining plots

# define a colour palette for the MATH taxonomy
MATH_colours <- c("A" = "#D81C3F", "B" = "#0070C0", "C" = "#00B050")
```

# Data

Information about the test:

```{r load-test-info, echo=FALSE, message=FALSE}
item_info <- read_csv("data-uoe/edinburgh-diagtest-metadata.csv") %>% 
  select(question = pre, description, MATH_group) %>% 
  filter(!is.na(question))

item_info %>%
  gt() %>%
  data_color(
    columns = contains("MATH"),
    colors = MATH_colours
  )
```

Load the student scores for the test:

```{r load-all-data, echo=FALSE, message=FALSE}
# TODO - this file has multiple years, with no indication of which year each attempt comes from; it would be good to add that info
test_scores <-
  vroom(
    fs::dir_ls(path = "data-uoe", glob = "*diagtest_detail*"),
    # Moodle records "-" as the result for questions that were not attempted,
    # so we set vroom's na option to interpret this as NA
    na = c("-", "", "NA"), 
    id = "cohort" 
  ) %>%
  mutate(year = str_remove(cohort, "data-uoe/ANON_") %>% str_remove(., "diagtest_detail.csv"))%>% 
  filter(!is.na(Total)) %>%
  rename_with(.cols = starts_with("Q"), .fn = ~ str_replace(., "Q", "A")) %>% 
  select(year, AnonID, Total, starts_with("A"))

test_scores_unfiltered <- test_scores
```

<details>
    <summary>Show data summary</summary>
```{r data-peek}
test_scores %>% skim()
```
</details>


## Data cleaning

Included in the data are many abandoned attempts, where students have apparently not engaged with most questions (e.g. after the first few on the test). Unfortunately the data only includes a score for each item, and not whether it was actually answered, so to try to remove these "non-serious" attempts, we use a process of eliminating based on scores in the latter half of the test:

1. For students who took the test more than once, consider the attempt with the highest scores only and remove the others;

2. Eliminate the students who scored three or more zeros in the 5 easiest questions in the
second-half of the test; and

3. Add the students scoring more than 30 marks in total back to the sample.

```{r fig.cap="Distribution of scores, filtered vs unfiltered"}
test_scores <- test_scores_unfiltered %>% 
  group_by(AnonID) %>% 
  slice_max(Total, n = 1) %>% 
  ungroup() %>% 
  rowwise() %>% 
  mutate(zeros_in_easiest_5 = sum(A11==0, A12==0, A13==0, A16==0, A17==0)) %>% 
  filter(zeros_in_easiest_5 <= 2 | Total >= 30) %>% 
  select(-zeros_in_easiest_5) %>% 
  ungroup()

bind_rows(
  "unfiltered" = test_scores_unfiltered %>% select(Total),
  "filtered" = test_scores %>% select(Total),
  .id = "dataset"
) %>% 
  group_by(dataset) %>% 
  # add n's to the facet titles
  mutate(dataset = str_glue("{dataset} (n={n()})") %>% as_factor()) %>% 
  # flip them so that filtered appears on the right
  mutate(dataset = fct_rev(dataset)) %>% 
  ggplot(aes(x = Total)) +
  geom_histogram() +
  facet_wrap(vars(dataset)) +
  theme_minimal()
```


## Data summary

Summary of each cohort:

```{r skim-classes}
test_scores_summary <- test_scores %>% 
  group_by(year) %>% 
  summarise(
    n = n(),
    mean = mean(Total),
    sd = sd(Total),
    median = median(Total)
  )

test_scores_summary %>% 
  gt() %>% 
  fmt_number(columns = c("mean", "sd"), decimals = 2) %>%
  data_color(
    columns = c("n"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  )
```

```{r}
test_scores %>% 
  ggplot(aes(x = Total)) +
  ggridges::geom_density_ridges(aes(y = year, fill = year)) +
  #facet_grid(cols = vars(year)) +
  theme_minimal()
```

```{r}
p1 <- test_scores %>% 
  ggplot(aes(x = Total)) +
  geom_histogram(binwidth = 5) +
  #scale_x_continuous(limits = c(0,100), breaks = c(0, 50, 100)) +
  facet_grid(cols = vars(year)) +
  theme_minimal() +
  labs(x = "Total score (out of 100)",
       y = "Number of students",
       title = "Edinburgh MDT") +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank())

p2 <- test_scores_summary %>% 
  mutate(
    n = str_glue("{n}"),
    mean = str_glue("{round(mean, digits = 1)}"),
    sd = str_glue("{round(sd, digits = 1)}"),
    median = str_glue("{median}")
  ) %>% 
  pivot_longer(c(n, mean, sd, median), names_to = "layer", values_to = "label") %>% 
  mutate(layer = fct_relevel(layer, c("n", "sd", "mean", "median")) %>% fct_rev()) %>% 
  ggplot(aes(x = 80, y = layer, label = label)) +
  geom_text(size = 10 * 5/14, hjust = 1) +
  scale_x_continuous(limits = c(0,100)) +
  facet_grid(cols = vars(year)) +
  labs(y = "", x = NULL) +
  scale_y_discrete(labels = c("n" = "N", "mean" = "Mean", "median" = "Median")) +
  theme_minimal() +
  theme(axis.line = element_blank(), axis.ticks = element_blank(), axis.text.x = element_blank(),
        panel.grid = element_blank(), strip.text = element_blank())

p1 / p2 +  plot_layout(heights = c(5, 2.5))

ggsave("output/uoe_pre_data-summary.pdf", units = "cm", width = 12, height = 8)
```


Mean and standard deviation for each item:

```{r skim-all-data}
test_scores %>% 
  select(-AnonID, -Total) %>% 
  group_by(year) %>% 
  skim_without_charts() %>% 
  select(-contains("character."), -contains("numeric.p"), -skim_type) %>% 
  rename(complete = complete_rate) %>% 
  # make the table wider, i.e. with separate columns for each year's results, with the year at the start of each column name
  pivot_wider(names_from = year, values_from = -c(skim_variable, year), names_glue = "{year}__{.value}") %>% 
  # put the columns in order by year
  select(sort(names(.))) %>% 
  select(skim_variable, everything()) %>% 
  # use GT to make the table look nice
  gt(rowname_col = "skim_variable") %>% 
  # group the columns from each year
  tab_spanner_delim(delim = "__") %>%
  fmt_number(columns = contains("numeric"), decimals = 2) %>%
  fmt_percent(columns = contains("complete"), decimals = 0) %>% 
  # change all the numeric.mean and numeric.sd column names to Mean and SD
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.mean"), label = "Mean") %>% deframe()
  ) %>% 
  cols_label(
    .list = test_scores %>% select(year) %>% distinct() %>% transmute(col = paste0(year, "__numeric.sd"), label = "SD") %>% deframe()
  ) %>%
  data_color(
    columns = contains("numeric.mean"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  )
```



# Testing assumptions

Before applying IRT, we should check that the data satisfies the assumptions needed by the model. In particular, to use a 1-dimensional IRT model, we should have some evidence of unidimensionality in the test scores.

## Inter-item correlations

If the test is unidimensional then we would expect student scores on pairs of items to be correlated.

This plot shows the correlations between scores on each pair of items:

```{r corr-plot}
item_scores <- test_scores %>% 
  select(starts_with("A"), -AnonID)

cor_ci <- psych::corCi(item_scores, plot = FALSE)

psych::cor.plot.upperLowerCi(cor_ci)
```

Checking for correlations that are not significantly different from 0, there are none:

```{r cor-not-corr}
cor_ci$ci %>% 
  as_tibble(rownames = "corr") %>% 
  filter(p > 0.05) %>% 
  arrange(-p) %>% 
  select(-contains(".e")) %>% 
  gt() %>% 
  fmt_number(columns = 2:4, decimals = 3)
```

The overall picture is that the item scores are well correlated with each other.

## Dimensionality

```{r fa-checks}
structure <- check_factorstructure(item_scores)
n <- n_factors(item_scores)
```

```{r fa-structure-check, echo=FALSE, results="asis"}
# check_factorstructure(item_scores)

# HACK - to make the heading printed by easystats be h4 rather than h1, use ### 
# TODO - perhaps suggest modification to https://github.com/easystats/insight/blob/master/R/print.easystats_check.R and https://github.com/easystats/parameters/blob/cbbe89c469148735110d5c16ef153e72a20bb0a0/R/n_factors.R#L352

res <- capture.output(structure)
cat(paste0("##", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors, echo=FALSE, results = "asis"}
res <- capture.output(n)
cat(paste0("##", paste0(res, collapse = "\n"), sep = ""))
```

```{r fa-num-factors-details}
plot(n)
summary(n) %>% gt()
#n %>% tibble() %>% gt()
```


```{r fa-scree}
fa.parallel(item_scores, fa = "fa")
```
```{r include=FALSE}
pdf(file = "output/uoe_pre_scree.pdf", width = 6, height = 4)
fa.parallel(item_scores, fa = "fa")
dev.off()
```

### 1 Factor

We use the `factanal` function to fit a 1-factor model.

_Note that this function cannot handle missing data, so any `NA` scores must be set to `0` for this analysis._

```{r fa1, cache=TRUE}
fitfact <- factanal(item_scores,
                    factors = 1,
                    rotation = "varimax")
print(fitfact, digits = 2, cutoff = 0.3, sort = TRUE)

load <- tidy(fitfact)

load %>% 
  select(question = variable, factor_loading = fl1) %>% 
  left_join(item_info, by = "question") %>% 
  ggplot(aes(x = factor_loading, y = 0, colour = MATH_group)) + 
    geom_point() + 
    geom_label_repel(aes(label = question), show.legend = FALSE) +
    scale_colour_manual("MATH group", values = MATH_colours) +
  scale_y_discrete() +
    labs(x = "Factor 1", y = NULL,
         title = "Standardised Loadings", 
         subtitle = "Based on 1-factor solution") +
    theme_minimal()
```

```{r}
load %>% 
  select(question = variable, factor_loading = fl1) %>% 
  left_join(item_info, by = "question") %>% 
  arrange(-factor_loading) %>% 
  gt() %>%
  data_color(
    columns = contains("factor"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("MATH"),
    colors = MATH_colours
  )
```

It is striking here that the MATH Group B questions are those that load most strongly onto this factor.


### 4 Factor

Here we also investigate the 4-factor solution, to see whether these factors are interpretable.

```{r fa-alt, cache=TRUE}
fitfact2 <- factanal(item_scores, factors = 4, rotation = "varimax")
print(fitfact2, digits = 2, cutoff = 0.3, sort = TRUE)

load2 <- tidy(fitfact2)

load2_plot <- load2 %>%
  rename(question = variable) %>% 
  left_join(item_info, by = "question") %>%
  ggplot(aes(x = fl1, y = fl2, colour = MATH_group, shape = MATH_group)) +
  geom_point() +
  geom_text_repel(aes(label = question), show.legend = FALSE, alpha = 0.6) +
  labs(
    x = "Factor 1 (of 4)",
    y = "Factor 2 (of 4)"
  ) +
  scale_colour_manual("MATH group", values = MATH_colours[1:2]) +
  scale_shape_manual(name = "MATH group", values = c(19, 17)) +
  theme_minimal()

load2_plot +
  labs(
    title = "Standardised Loadings",
    subtitle = "Showing the first 2 factors of the 4-factor model"
  )

ggsave("output/uoe_pre_4factor.pdf", units = "cm", width = 14, height = 10, dpi = 300,
       plot = load2_plot)
```

```{r}
main_factors <- load2 %>% 
#  mutate(factorNone = 0.4) %>%  # add this to set the main factor to "None" where all loadings are below 0.4
  pivot_longer(names_to = "factor",
               cols = contains("fl")) %>% 
  mutate(value_abs = abs(value)) %>% 
  group_by(variable) %>% 
  top_n(1, value_abs) %>% 
  ungroup() %>% 
  transmute(main_factor = factor, variable)


load2 %>% 
  select(-uniqueness) %>% 
  # add the info about which is the main factor
  left_join(main_factors, by = "variable") %>%
  left_join(item_info %>% select(variable = question, description, MATH_group), by = "variable") %>% 
  arrange(main_factor) %>% 
  select(main_factor, everything()) %>% 
  # arrange adjectives by descending loading on main factor
  rowwise() %>% 
  mutate(max_loading = max(abs(c_across(starts_with("fl"))))) %>% 
  group_by(main_factor) %>% 
  arrange(-max_loading, .by_group = TRUE) %>% 
  select(-max_loading) %>% 
  # sort out the presentation
  rename("Main Factor" = main_factor,
         "Question" = variable) %>%
  mutate_at(
    vars(starts_with("fl")),
    ~ cell_spec(round(., digits = 3), bold = if_else(abs(.) > 0.4, T, F))
  ) %>% 
  kable(booktabs = T, escape = F, longtable = T) %>% 
  kableExtra::collapse_rows(columns = 1, valign = "top") %>%
  kableExtra::kable_styling(latex_options = c("repeat_header"))
```

The first factor is dominated by questions that had previously been identified as MATH Group B, i.e. those that are somehow "non-standard" -- either requiring students to recognise that a particular rule/procedure is applicable before applying it, or to apply it in an unusual way. This factor also includes Group A questions on "pre-calculus" topics (such as fractions, logarithms and trigonometry) that students had perhaps not explicitly practiced most recently.

The second factor is dominated by the two chain rule questions (A16 and A17), along with A18 which is a routine definite integral, suggesting this factor is related to routine calculus computations.

The third factor seems to be based on applying calculus techniques to cubic and quadratic curves, e.g. to find tangent lines or stationary points.

The fourth factor is dominated by the only two questions that require the use of a calculator (to compute trigonometric functions), but more generally seems to be based on non-calculus skills (vectors, trig, sequences).


# Fitting IRT model

The `mirt` implementation of the graded partial credit model (`gpmc`) requires that the partial marks are consecutive integers. We therefore need to work around this by adjusting our scores into that form (e.g. replacing scores of 0, 2.5, 5 with 1, 2, 3), while keeping track of the true scores attached to each mark level so that we can properly compute expected scores later on.

```{r}
# Determine the mark levels for each item
mark_levels <- item_scores %>% 
  pivot_longer(everything(), names_to = "item", values_to = "score") %>% 
  distinct() %>% 
  arrange(parse_number(item), score) %>% 
  group_by(item) %>%
  mutate(order = row_number()) %>% 
# Note that the convention used by mirt is for items that have only 2 levels (i.e. 0 marks or full marks),
# the columns are P.0 and P.1, while other items are indexed from 1, i.e. P.1, P.2, ...
# https://github.com/philchalmers/mirt/blob/accd2383b9a4d17a4cab269717ce98434900b62c/R/probtrace.R#L57
  mutate(level = case_when(
    max(order) == 2 ~ order - 1,
    TRUE ~ order * 1.0
  )) %>% 
  mutate(levelname = paste0(item, ".P.", level))

# Use the mark_levels table to replace scores with levels
# (first pivot the data to long form, make the replacement, then pivot back to wide again)
item_scores_levelled <- item_scores %>% 
  # temporarily add row identifiers
  mutate(row = row_number()) %>% 
  pivot_longer(cols = -row, names_to = "item", values_to = "score") %>% 
  left_join(mark_levels %>% select(item, score, level), by = c("item", "score")) %>% 
  select(-score) %>% 
  pivot_wider(names_from = "item", values_from = "level") %>% 
  select(-row)
```


<details>
    <summary>Show model fitting output</summary>
```{r fit-mirt, warning=FALSE, message=FALSE, cache=TRUE}
fit_gpcm <- mirt(
  data = item_scores_levelled, # just the columns with question scores
  model = 1,          # number of factors to extract
  itemtype = "gpcm",  # generalised partial credit model
  SE = TRUE           # estimate standard errors
  )
```
</details>


## Local independence

We compute Yen's $Q_3$ (1984) to check for any dependence between items after controlling for $\theta$. This gives a score for each pair of items, with scores above 0.2 regarded as problematic (see DeMars, p. 48).

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
residuals <- residuals(fit_gpcm, type = "Q3") %>% data.frame
```
```{r}
residuals  %>% as.matrix() %>% 
  corrplot::corrplot(type = "upper")
```

This shows that most item pairs are independent, with only one pair showing cause for concern:

```{r}
residuals %>%
  rownames_to_column(var = "item1") %>%
  as_tibble() %>% 
  pivot_longer(cols = starts_with("A"), names_to = "item2", values_to = "Q3_score") %>% 
  filter(abs(Q3_score) > 0.2) %>% 
  filter(parse_number(item1) < parse_number(item2)) %>%
  gt()
```

Items A16 and A17 are on the chain rule (e.g. differentiating $\cos(4x^2+5)$ and $(3x^2-8)^3$ respectively), so it is perhaps unsurprising that students' performance on these items is not entirely independent.

Given that this violation of the local independence assumption is very mild, we proceed using this model.

## Model parameters

We augment the data with estimated abilities for each student, using mirt's `fscores()` function.

```{r augment-with-f1}
test_scores_with_ability <- test_scores %>%
  mutate(F1 = fscores(fit_gpcm, method = "EAP"))
```

Next, we extract the IRT parameters.

```{r extract-coefs}
coefs_gpcm <- coef(fit_gpcm, IRTpars = TRUE)
```

We use the `tidy_mirt_coeffs` function to get all the parameter estimates into a tidy table:

```{r tidy-mirt-coefs}
source("fn_tidy_mirt_coefs.R")

tidy_gpcm <- tidy_mirt_coefs(coefs_gpcm)
```

```{r}
tidy_gpcm %>% 
  filter(par == "a") %>% 
  select(-par) %>% 
  rename_with(.fn = ~ paste0("a_", .x), .cols = -Question) %>% 
  left_join(
    tidy_gpcm %>% 
      filter(str_detect(par, "^b")),
    by = "Question"
  ) %>% 
  gt(groupname_col = "Question") %>%
  fmt_number(columns = contains("est|_"), decimals = 3) %>%
  data_color(
    columns = contains("a_"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = c("est", "CI_2.5", "CI_97.5"),
    colors = scales::col_numeric(palette = c("Blues"), domain = NULL)
  ) %>%
  tab_spanner(label = "Discrimination", columns = contains("a_")) %>%
  tab_spanner(label = "Difficulty", columns = c("par", "est", "CI_2.5", "CI_97.5"))
```


```{r save-results}
tidy_gpcm %>% 
  write_csv("output/uoe_pre_gpcm-results.csv")
```


## Information curves

```{r}
theta <- seq(-6, 6, by=0.05)

info_matrix <- testinfo(fit_gpcm, theta, individual = TRUE)
colnames(info_matrix) <- item_info %>% pull(question)
item_info_data <- info_matrix %>% 
  as_tibble() %>% 
  bind_cols(theta = theta) %>% 
  pivot_longer(cols = -theta, names_to = "item", values_to = "info_y") %>% 
  left_join(item_info %>% select(item = question, MATH_group), by = "item") %>% 
  mutate(item = fct_reorder(item, parse_number(item)))
```

### Test information curve

```{r}
item_info_data %>% 
  group_by(theta) %>% 
  summarise(info_y = sum(info_y)) %>% 
  ggplot(aes(x = theta, y = info_y)) +
  geom_line() +
  labs(x = "Ability", y = "Information", title = "Edinburgh MDT") +
  theme_minimal()

ggsave("output/uoe_pre_info.pdf", width = 10, height = 6, units = "cm")
```

This shows that the information given by the test is skewed toward the lower end of the ability scale - i.e. it can give more accurate estimates of students' ability where their ability level is slightly below the mean.

### Item information curves

Breaking this down by question helps to highlight those questions that are most/least informative:

```{r}
item_info_data %>% 
  ggplot(aes(x = theta, y = info_y, colour = item)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  facet_wrap(vars(item)) +
  labs(y = "Information") +
  theme_minimal()
```

We can also compute the sums of different subsets of the information curves -- here, we will look at the questions based on their MATH group:

```{r fig.height=3, fig.width=4}
item_info_data %>% 
  group_by(theta) %>% 
  summarise(
    items_all = sum(info_y),
    items_A = sum(ifelse(MATH_group == "A", info_y, 0)),
    items_B = sum(ifelse(MATH_group == "B", info_y, 0))
  ) %>% 
  pivot_longer(cols = starts_with("items_"), names_to = "items", names_prefix = "items_", values_to = "info_y") %>% 
  mutate(items = fct_relevel(items, "all", "A", "B")) %>% 
  ggplot(aes(x = theta, y = info_y, colour = items)) +
  geom_line() +
  scale_colour_manual(values = c("all" = "#000000", MATH_colours)) +
  labs(x = "Ability", y = "Information") +
  theme_minimal()

ggsave("output/uoe_pre_info-curves_A-vs-B.pdf", units = "cm", width = 14, height = 6)
```

This shows that the information in the MATH Group B questions is at a higher point on the ability scale than for the MATH Group A questions.

Since the number of items in each case is different, we consider instead the mean information per item:

```{r fig.height=3, fig.width=4}
item_info_data %>% 
  group_by(theta) %>% 
  summarise(
    items_all = sum(info_y) / n(),
    items_A = sum(ifelse(MATH_group == "A", info_y, 0)) / sum(ifelse(MATH_group == "A", 1, 0)),
    items_B = sum(ifelse(MATH_group == "B", info_y, 0)) / sum(ifelse(MATH_group == "B", 1, 0))
  ) %>% 
  pivot_longer(cols = starts_with("items_"), names_to = "items", names_prefix = "items_", values_to = "info_y") %>% 
  mutate(items = fct_relevel(items, "all", "A", "B")) %>% 
  ggplot(aes(x = theta, y = info_y, colour = items)) +
  geom_line() +
  scale_colour_manual(values = c("all" = "#000000", MATH_colours)) +
  labs(x = "Ability", y = "Mean information per item") +
  theme_minimal()

ggsave("output/uoe_pre_info-curves_A-vs-B-avg.pdf", units = "cm", width = 10, height = 6)
```

This shows that items of each MATH group are giving broadly similar levels of information on average, but at different points on the ability scale.

## Total information

Using `mirt`'s `areainfo` function, we can find the total area under the information curves.

```{r}
info_gpcm <- areainfo(fit_gpcm, c(-4,4))
info_gpcm %>% gt()
```

This shows that the total information in all items is `r info_gpcm$TotalInfo`.

### Information by item

```{r}
tidy_info <- item_info %>%
  mutate(item_num = row_number()) %>% 
  mutate(TotalInfo = purrr::map_dbl(
    item_num,
    ~ areainfo(fit_gpcm,
               c(-4, 4),
               which.items = .x) %>% pull(TotalInfo)
  ))

tidy_info %>%
  select(-item_num) %>% 
  arrange(-TotalInfo) %>% 
  #group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("info"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("outcome"),
    colors = scales::col_factor(palette = c("viridis"), domain = NULL)
  ) %>%
  cols_label(
    TotalInfo = "Information"
  )
```

Restricting instead to the range $-2\leq\theta\leq2$:

```{r}
tidy_info <- item_info %>%
  mutate(item_num = row_number()) %>% 
  mutate(TotalInfo = purrr::map_dbl(
    item_num,
    ~ areainfo(fit_gpcm,
               c(-2, 2),
               which.items = .x) %>% pull(Info)
  ))

tidy_info %>%
  select(-item_num) %>% 
  arrange(-TotalInfo) %>% 
  #group_by(outcome) %>% 
  gt() %>% 
  fmt_number(columns = contains("a_"), decimals = 2) %>%
  fmt_number(columns = contains("b_"), decimals = 2) %>%
  data_color(
    columns = contains("info"),
    colors = scales::col_numeric(palette = c("Greens"), domain = NULL)
  ) %>%
  data_color(
    columns = contains("outcome"),
    colors = scales::col_factor(palette = c("viridis"), domain = NULL)
  ) %>%
  cols_label(
    TotalInfo = "Information"
  )
```
## Response curves

Since the `gpcm` model is more complicated, there is a characteristic curve for each possible score on the question:

```{r}
trace_data <- probtrace(fit_gpcm, theta) %>% 
  as_tibble() %>% 
  bind_cols(theta = theta) %>% 
  pivot_longer(cols = -theta, names_to = "level", values_to = "y") %>% 
  left_join(mark_levels %>% select(item, level = levelname, score), by = "level") %>% 
  mutate(score = as.factor(score))

trace_data %>% 
  mutate(item = fct_reorder(item, parse_number(item))) %>% 
  ggplot(aes(x = theta, y = y, colour = score)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  facet_wrap(vars(item)) +
  labs(y = "Probability of response") +
  theme_minimal()
```

To get a simplified picture for each question, we compute the expected score at each ability level:

```{r}
expected_scores <- trace_data %>% 
  mutate(item = fct_reorder(item, parse_number(item))) %>% 
  group_by(item, theta) %>% 
  summarise(expected_score = sum(as.double(as.character(score)) * y), .groups = "drop")

expected_scores %>% 
  ggplot(aes(x = theta, y = expected_score, colour = item)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  facet_wrap(vars(item)) +
  labs(y = "Expected score") +
  theme_minimal()
```

The resulting curves look quite similar to those from the 2PL, allowing for some similar interpretation. For instance, superimposing all the curves shows that there is a spread of difficulties (i.e. thetas where the expected score is 2.5/5) and that some questions are more discriminating than others (i.e. steeper slopes):

```{r}
plt <- expected_scores %>% 
  ggplot(aes(x = theta, y = expected_score, colour = item, text = item)) +
  geom_line() +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  labs(y = "Expected score") +
  theme_minimal()

ggplotly(plt, tooltip = "text")

ggsave(plot = plt, file = "output/uoe_pre_iccs-superimposed.pdf", width = 20, height = 14, units = "cm")
```

```{r}
highlight_removed_items <- expected_scores %>% 
  mutate(highlight_item = item %in% c("A2", "A8", "A11")) %>% 
  mutate(line_width = ifelse(highlight_item, 1, 0.5))

highlight_removed_items %>% 
  ggplot(aes(x = theta, y = expected_score, colour = item, text = item, alpha = highlight_item)) +
  geom_line(aes(size = highlight_item)) +
  #geom_point(data = highlight_removed_items %>% filter(highlight_item == TRUE, theta == 0)) +
  ggrepel::geom_label_repel(
    data = highlight_removed_items %>% filter(highlight_item == TRUE, theta == 0),
    aes(label = item),
    box.padding = 0,
    show.legend = FALSE
  ) +
  scale_colour_viridis_d("Question", option = "plasma", end = 0.8, direction = -1) +
  scale_size_manual(values = c("FALSE" = 0.6, "TRUE" = 0.9), guide = "none") +
  scale_alpha_discrete(guide = "none", range = c(0.2, 1)) +
  labs(x = "Ability", y = "Expected score") +
  theme_minimal() +
  theme(legend.position="bottom",#legend.title=element_blank(),
      legend.margin = margin(0, 0, 0, 0),
      legend.spacing.x = unit(1, "mm"),
      legend.spacing.y = unit(0, "mm")) +
  guides(colour = guide_legend(nrow = 2))
ggsave(file = "output/uoe_pre_iccs-highlight.pdf", width = 16, height = 10, units = "cm")
```


### Test response curve

```{r}
total_expected_score <- expected_scores %>% 
  group_by(theta) %>% 
  summarise(expected_score = sum(expected_score))

total_expected_score %>% 
  ggplot(aes(x = theta, y = expected_score)) +
  geom_line() +
  # geom_point(data = total_expected_score %>% filter(theta == 0)) +
  # ggrepel::geom_label_repel(data = total_expected_score %>% filter(theta == 0), aes(label = round(expected_score, 1)), box.padding = 0.5) +
  scale_colour_viridis_d(option = "plasma", end = 0.8, direction = -1) +
  labs(y = "Expected score") +
  theme_minimal()
```


# Predictive validity

```{r message=FALSE, warning=FALSE}
course_results <- read_csv("data-uoe/ANON_2013-2017_course-results.csv", col_types = "ccddddddddd")

course_results_long <- course_results %>% 
  pivot_longer(cols = !c(AnonID, year), names_to = "course", values_to = "mark") %>% 
  filter(!is.na(mark)) %>% 
  separate(course, into = c("course_type", "course"), sep = "_") %>% 
  mutate(year = str_replace(year, "/", "-1"))

course_results_vs_diagtest <- course_results_long %>% 
  left_join(test_scores_with_ability %>% select(AnonID, year, diagtest_score = Total, F1), by = c("AnonID", "year")) %>% 
  filter(!is.na(diagtest_score))
```

We have both course results and diagnostic test scores for the following number of students:

```{r}
course_results_vs_diagtest %>% 
  select(AnonID, year) %>% 
  distinct() %>% 
  tally()
```

## Specialist courses

Mathematics students take linear algebra (ILA) in semester 1, then calculus (CAP) and a proofs course (PPS) in semester 2.

```{r}
course_results_vs_diagtest %>% 
  filter(course_type == "spec") %>% 
  janitor::tabyl(year, course) %>% 
  gt()
```

```{r}
course_results_vs_diagtest %>% 
  filter(course_type == "spec") %>% 
  mutate(course = fct_relevel(course, "ILA", "CAP", "PPS")) %>% 
  ggplot(aes(x = diagtest_score, y = mark)) +
  geom_point(size = 0.8, stroke = 0, alpha = 0.5) +
  geom_smooth(method = lm, formula = "y ~ x") +
  ggpubr::stat_cor(label.y = 105, p.accuracy = 0.001) +
  facet_grid(cols = vars(course)) +
  theme_minimal() +
  theme(strip.text.x = element_text(size = 12)) +
  labs(x = "Edinburgh MDT score", y = "Course result")

ggsave("output/uoe_pre_regression-spec.pdf", units = "cm", width = 16, height = 8)
```

This shows that the diagnostic test is moderately predictive of success in year 1.

* Note that the correlations are lower in semester 2 courses; this is to be expected, as the information from the diagnostic test becomes less relevant as students experience more teaching during their courses.
* The correlation in PPS is lower than in CAP. Again this is as we would expect, since much of PPS is concerned with writing proofs and this is not something addressed by the diagnostic test.

## Non-specialist courses

On the non-specialist side, students take the following courses:

* MSE - for engineering and chemistry students, discontinued after 2014-15,
* EM - for engineering students
* MNS - for chemistry students

The courses come in two parts: 1a in semester 1, and 1b in semester 2.

```{r}
course_results_vs_diagtest %>% 
  filter(course_type == "nonspec") %>% 
  janitor::tabyl(year, course) %>% 
  gt()
```

```{r}
course_results_vs_diagtest %>% 
  filter(course_type == "nonspec") %>% 
  separate(course, into = c("course", "semester"), sep = "\\d") %>% 
  mutate(semester = ifelse(semester == "a", "Semester 1", "Semester 2")) %>% 
  mutate(course = fct_relevel(course, "MSE", "EM", "MNS")) %>% 
  ggplot(aes(x = diagtest_score, y = mark)) +
  geom_point(size = 0.8, stroke = 0, alpha = 0.5) +
  geom_smooth(method = lm, formula = "y ~ x") +
  ggpubr::stat_cor(label.y = 105, p.accuracy = 0.001) +
  facet_grid(rows = vars(semester), cols = vars(course)) +
  theme_minimal() +
  theme(strip.text.x = element_text(size = 12)) +
  labs(x = "Edinburgh MDT score", y = "Course result")

ggsave("output/uoe_pre_regression-nonspec.pdf", units = "cm", width = 16, height = 16)
```

We see a similar pattern of the diagnostic test being moderately predictive of Semester 1 results, and still somewhat predictive in Semester 2.

# About this report {.unnumbered}

This report supports the analysis in the following paper:

> [citation needed]

## Packages {.unnumbered}

In this analysis we used the following packages. You can learn more about each one by clicking on the links below.

- [**mirt**](https://cran.r-project.org/web/packages/mirt/mirt.pdf): For IRT analysis
- [**psych**](https://personality-project.org/r/psych/): For factor analysis
- [**tidyverse**](https://tidyverse.org/): For data wrangling and visualisation
- [**reshape**](http://had.co.nz/reshape/): For reshaping nested lists
- [**vroom**](https://vroom.r-lib.org/): For reading in many files at once
- [**broom**](https://broom.tidymodels.org/): For tidying model output
- [**fs**](https://fs.r-lib.org/): For file system operations
- [**gt**](https://gt.rstudio.com/): For formatting tables
- [**knitr**](https://yihui.org/knitr/): For markdown tables
- [**ggrepel**](https://ggrepel.slowkow.com/): For labelling points without overlap
- [**skimr**](https://docs.ropensci.org/skimr/): For data frame level summary
- [**ggridges**](https://wilkelab.org/ggridges/): For ridge plots


